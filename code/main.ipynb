{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes\n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "url = 'https://myanimelist.net/topanime.php'\n",
    "response= requests.get(url)\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup.find_all('a') #Finds all the links\n",
    "#soup.find_all('tr') # Finds all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_soup(soup):\n",
    "    anime = []\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:\n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime.append((link.contents[0], link.get('href')))\n",
    "    return anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_list= []\n",
    "for lim in range(0, 20000, 50):\n",
    "    if lim==0:\n",
    "        new_url = url\n",
    "    else:\n",
    "        new_url = url+'?limit='+str(lim)\n",
    "    response = requests.get(new_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tot_list += get_links_from_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'w') as f:\n",
    "    for n, link in tot_list:\n",
    "        f.write(link+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "* Download the html corresponding to each of the collected urls.\n",
    "* After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the * * * stopping point. More details in Important (2).\n",
    "* Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\") as file:\n",
    "    #the line will be the url\n",
    "    number=0\n",
    "    \n",
    "    for url in file:\n",
    "        respone=requests.get(url)\n",
    "        soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "        soup=str(soup)\n",
    "        #save file as htlm\n",
    "        name=('article_%d.html' % (number))\n",
    "        html_file = open(name,\"w\")\n",
    "        html_file.write(soup)\n",
    "        html_file.close()\n",
    "        number+=1\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Due to the large amount of pages you need to download, follow the next tips that will help you speeding up several time-consuming operations.\n",
    "\n",
    "[Save time downloading files] You are asked to crawl a considerable number of pages, which will take plenty of time. To speed up the operation, we suggest you to work in parallel with your group's colleagues or even generate code that works in parallel with all the CPUs available in your computer. In particular, using the same code, each component of the group can be in charge of downloading a subset of pages (e.g., the first 100). PAY ATTENTION: Once obtained all the pages, merge your results into an unique dataset. In fact, the search engine must look up for results in the whole set of documents.\n",
    "\n",
    "[Save your data] It is not nice to restart a crawling procedure, given its runtime. For this reason, it is extremely important that for every time you crawl a page, you must save it with the name article_i.html, where i corresponds to the number of articles you have already downloaded. In such way, if something goes bad, you can restart your crawling procedure from the i+1-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "* **Anime Name** (to save as animeTitle): *String*\n",
    "* **Anime Type** (to save as animeType): *String*\n",
    "* **Number of episode** (to save as animeNumEpisode): *Integer*\n",
    "* **Release and End Dates of anime** (to save as releaseDate and endDate): Convert both release and end date into *datetime format*.\n",
    "* **Number of members** (to save as animeNumMembers): *Integer*\n",
    "* **Score** (to save as animeScore): *Float*\n",
    "* **Users** (to save as animeUsers): *Integer*\n",
    "* **Rank** (to save as animeRank): *Integer*\n",
    "* **Popularity** (to save as animePopularity): *Integer*\n",
    "* **Synopsis** (to save as animeDescription): *String*\n",
    "* **Related Anime** (to save as animeRelated): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. *List of strings*.\n",
    "* **Characters** (to save as animeCharacters): *List of strings*.\n",
    "* **Voices** (to save as animeVoices): *List of strings*\n",
    "* **Staff** (to save as animeStaff): Include the staff name and their responsibility/task in a *list of lists*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each anime, you create an anime_i.tsv file of this structure:\n",
    "\n",
    "animeTitle \\t animeType \\t  ... \\t animeStaff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SEARCH ENGINE\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the animes that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each anime by:\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Stemming\n",
    "* Anything else you think it's needed\n",
    "\n",
    "For this purpose, you can use the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ninakaploukhaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ninakaploukhaya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"../shared_stuff/tsv_files/total_pages.tsv\",\n",
    "                       delimiter = \"\\t\",\n",
    "                       header = \"infer\",\n",
    "                       on_bad_lines = \"skip\") # fix line 16473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s):\n",
    "    \"\"\"\n",
    "    This functions returns a list\n",
    "    with each of the characters of the input string\n",
    "    \n",
    "    Arguments\n",
    "        s : string\n",
    "        \n",
    "    Returns\n",
    "        (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    return [char for char in s]\n",
    "\n",
    "def has_digits(s):\n",
    "    \"\"\"\n",
    "    This function checks whether a string\n",
    "    contains any digits\n",
    "    \n",
    "    Arguments\n",
    "        s : string\n",
    "        \n",
    "    Returns\n",
    "        (bool) True / False\n",
    "    \"\"\"\n",
    "    \n",
    "    return len([char for char in s if char.isdigit()]) != 0\n",
    "\n",
    "def bad_words():\n",
    "    \"\"\"\n",
    "    This function creates a list with words\n",
    "    that should be excluded from the vocabulary\n",
    "    during preprocessing, including punctuation,\n",
    "    stopwords et similia\n",
    "    \n",
    "    Arguments\n",
    "        none\n",
    "        \n",
    "    Returns\n",
    "        (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    punct = str_to_list(string.punctuation)\n",
    "    punct += [\"...\", \"''\", \"``\", '\"\"']\n",
    "    \n",
    "    stops = stopwords.words(\"english\")\n",
    "    \n",
    "    other_suffixes = [\"'s\", \"n't\"]\n",
    "    \n",
    "    return punct + stops + other_suffixes\n",
    "\n",
    "def preprocess(text, stemmer):\n",
    "    \"\"\"\n",
    "    This function preprocesses some text (a document)\n",
    "    by isolating each word, excluding stopwords et similia,\n",
    "    and finally stemming them\n",
    "    \n",
    "    Arguments\n",
    "        text : (string)\n",
    "        stemmer : stemmer object, e.g. SnowBallStemmer()\n",
    "    \n",
    "    Returns\n",
    "        (list) preprocessed input text\n",
    "    \"\"\"\n",
    "    \n",
    "    text = str(text).replace(\"/\",\" \") #between some words there is /, let's replace it with space\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "        \n",
    "    return [stemmer.stem(w) for w in tokens \n",
    "            if w not in bad_words() and not has_digits(w) and len(w) == len(w.encode())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "df['synopsis_clean'] = df.apply(lambda row: preprocess(row['synopsis'], stemmer), \n",
    "                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!\n",
    "Before building the index,\n",
    "\n",
    "Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "where document_i is the id of a document that contains the word.\n",
    "\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth to think to store it in a separate file and load it in memory when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(corpus):\n",
    "    \"\"\"\n",
    "    This function creates a set of unique\n",
    "    and preprocessed words from a corpus\n",
    "    \n",
    "    Arguments\n",
    "        corpus : pandas df column or list-like\n",
    "    \n",
    "    Returns\n",
    "        vocab  : dictionary with the words as keys\n",
    "                 and a unique integer for each as values\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()\n",
    "    \n",
    "    for doc in corpus:\n",
    "        vocab.update(set(doc))\n",
    "\n",
    "    return {word:idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "\n",
    "def save_dict_to_file(dct, filename):\n",
    "    \"\"\"\n",
    "    This function saves a dictionary \n",
    "    to an external JSON file\n",
    "    \n",
    "    Arguments\n",
    "        dct       : dictionary\n",
    "        filename  : name of the file\n",
    "        \n",
    "    Returns\n",
    "        void\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(dct, file)\n",
    "        \n",
    "\n",
    "def read_dict_from_file(filename):\n",
    "    \"\"\"\n",
    "    This function reads a dictionary\n",
    "    from an external JSON file\n",
    "        \n",
    "    Arguments\n",
    "        filename : name of the file\n",
    "    \n",
    "    Returns\n",
    "        dct : dictionary with the contents of 'filename'\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        dct = json.loads(file.read())\n",
    "\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute this cell the first time or \n",
    "# when the preprocessing changes!\n",
    "\n",
    "vocab = create_vocab(df['synopsis_clean'])\n",
    "\n",
    "save_dict_to_file(vocab, \"vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = read_dict_from_file(\"vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inv_idx(corpus, vocab): \n",
    "    \"\"\"\n",
    "    This functions creates an inverted index list\n",
    "    given a corpus of documents and a vocabulary\n",
    "    \n",
    "    Arguments\n",
    "        corpus  : pandas df column or list-like\n",
    "        vocab   : dictionary of all the words in the corpus\n",
    "    \n",
    "    Returns\n",
    "        inv_idx : dictionary with the words as referenced in 'vocab' as keys \n",
    "                  and the lists of the documents each word is in as values       \n",
    "    \"\"\"\n",
    "    \n",
    "    inv_idx = {}\n",
    "    \n",
    "    for idx, word in zip(vocab.values(), vocab.keys()):\n",
    "        inv_idx[idx] = [doc_id for doc_id, doc in enumerate(corpus) if word in doc]\n",
    "    \n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute this cell the first time or \n",
    "# when the preprocessing/vocabulary change!\n",
    "\n",
    "inv_idx = create_inv_idx(df['synopsis_clean'], vocab)\n",
    "\n",
    "save_dict_to_file(inv_idx, \"inv_idx.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keys of the dict we get are str, not int like when we created it\n",
    "# I don't think it's necessary, but should we parse them when we read the json?\n",
    "inv_idx = read_dict_from_file(\"inv_idx.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "saiyan race\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "animeTitle\n",
    "animeDescription\n",
    "Url\n",
    "Example Output:\n",
    "\n",
    "animeTitle\tanimeDescription\tUrl\n",
    "Fullmetal Alchemist: Brotherhood\t...\thttps://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n",
    "Gintama\t...\thttps://myanimelist.net/anime/28977/Gintama%C2%B0\n",
    "Shingeki no Kyojin Season 3 Part 2\t...\thttps://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2\n",
    "If everything works well in this step, you can go to the next point, and make your Search Engine more complex and better in answering queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query, vocab, stemmer):\n",
    "    \"\"\"\n",
    "    This functions converts the list of words\n",
    "    input by the user into the list of the IDs\n",
    "    the words are saved as in the vocabulary\n",
    "    \n",
    "    Arguments\n",
    "        query   : list of words\n",
    "        vocab   : vocabulary of words with the words as keys\n",
    "                  and their IDs as values\n",
    "        stemmer : stemmer object, e.g. SnowBallStemmer()\n",
    "        \n",
    "    Returns\n",
    "        list of the IDs of the words in the query\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_query = []\n",
    "    \n",
    "    for word in query:\n",
    "        try:\n",
    "            parsed_query.append(vocab[stemmer.stem(word)])\n",
    "        except KeyError:\n",
    "            print(f\"The term '{word}' wasn't found anywhere!\")\n",
    "    \n",
    "    return parsed_query\n",
    "\n",
    "\n",
    "def get_results(query, inv_dx):\n",
    "    \"\"\"\n",
    "    This functions finds the documents all the words\n",
    "    in the query are in.\n",
    "    \n",
    "    It finds them in three steps:\n",
    "    1. creates a list of docs each word is in from the inverted index\n",
    "    2. converts that list into a set\n",
    "    3. intersects all those sets into a single set\n",
    "       \n",
    "    Arguments\n",
    "        query   : list of words as parsed by 'parse_query'\n",
    "        inv_idx : (dictionary) inverted index\n",
    "        \n",
    "    Returns\n",
    "        set with the documents that contain all the words in the query\n",
    "    \"\"\"\n",
    "    \n",
    "    return set.intersection(*[set(inv_idx[str(q)]) for q in query])\n",
    "\n",
    "\n",
    "def get_df_entries(df, results,\n",
    "                   url_file = \"../shared_stuff/url_list.txt\", \n",
    "                   simil = None):\n",
    "    \"\"\"\n",
    "    This function filters the dataset so it only shows\n",
    "    the rows which match the results, and adds a new column\n",
    "    with the URL for the anime of each row.\n",
    "    \n",
    "    Arguments\n",
    "        df       : pandas dataframe\n",
    "        results  : set with the row indices to be filtered out\n",
    "        url_file : external file with the URLs for each of the rows in df\n",
    "        simil    : dictionary with the similarity scores\n",
    "    \n",
    "    Returns\n",
    "        df : filtered pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    with open(url_file, 'r') as file:\n",
    "        url_list = file.read().split(\"\\n\")\n",
    "\n",
    "    df = df.iloc[[*results]]\n",
    "    df = df[[\"title\", \"synopsis\"]]\n",
    "    df = df.rename(columns = {\"title\": \"animeTitle\", \n",
    "                              \"synopsis\": \"animeDescription\"})\n",
    "    \n",
    "    df['animeUrl'] = itemgetter(*results)(url_list)\n",
    "    \n",
    "    if simil is not None: #in case we need a colum for similiaty\n",
    "        df[\"Similarity\"] = itemgetter(*results)(simil)\n",
    "        df = df.sort_values(by=['Similarity'], ascending = False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best anime\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15236</th>\n",
       "      <td>Choubakuretsu Ijigen Menko Battle: Gigant Shoo...</td>\n",
       "      <td>The anime follows Tsukasa, whose dream is to b...</td>\n",
       "      <td>https://myanimelist.net/anime/42875/Chou_Futsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14725</th>\n",
       "      <td>Ad Lib Anime Kenkyuujo</td>\n",
       "      <td>Emiri Katou and Kaori Fukuhara, the voice actr...</td>\n",
       "      <td>https://myanimelist.net/anime/48586/ABC_Tenkiy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510</th>\n",
       "      <td>Nurse Witch Komugi-chan Magikarte</td>\n",
       "      <td>Ungrar, the King of Viruses, has escaped from ...</td>\n",
       "      <td>https://myanimelist.net/anime/1300/Omishi_Maho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>Dr. Slump</td>\n",
       "      <td>In Penguin Village, humans live alongside talk...</td>\n",
       "      <td>https://myanimelist.net/anime/5197/Dragon_League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16909</th>\n",
       "      <td>Kuakou de Qingwa</td>\n",
       "      <td>The story is about a proud frog who likes to b...</td>\n",
       "      <td>https://myanimelist.net/anime/48011/Kuaile_Xia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10388</th>\n",
       "      <td>Tamala 2010: A Punk Cat in Space OVA</td>\n",
       "      <td>After a long 5-year wait, the team that create...</td>\n",
       "      <td>https://myanimelist.net/anime/36162/Sou__Uchia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13340</th>\n",
       "      <td>Shounen H ga Mita Sensou</td>\n",
       "      <td>An educational anime about the Pacific War. Ba...</td>\n",
       "      <td>https://myanimelist.net/anime/8829/Shounen_Ash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8607</th>\n",
       "      <td>Heybot!</td>\n",
       "      <td>The anime takes place on the screw-shaped isla...</td>\n",
       "      <td>https://myanimelist.net/anime/3906/H_P_Lovecra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14500</th>\n",
       "      <td>Yowai Robot to 10 no Story Project</td>\n",
       "      <td>The project is inspired by \"weak robot\", it is...</td>\n",
       "      <td>https://myanimelist.net/anime/21129/Youtai_Nuh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8359</th>\n",
       "      <td>Attraction</td>\n",
       "      <td>Interactive anti-smoking anime. Directed by Ko...</td>\n",
       "      <td>https://myanimelist.net/anime/2537/Angelique__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>Macross Flash Back 2012</td>\n",
       "      <td>Flash Back 2012 is Minmei's farewell concert. ...</td>\n",
       "      <td>https://myanimelist.net/anime/10370/Metal_Figh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10157</th>\n",
       "      <td>Guilstein</td>\n",
       "      <td>The theatrical anime film is a full digital (3...</td>\n",
       "      <td>https://myanimelist.net/anime/28077/Go_Go_575_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6324</th>\n",
       "      <td>Kimagure Mercy</td>\n",
       "      <td>Animated PV created for the release of Hachioj...</td>\n",
       "      <td>https://myanimelist.net/anime/4504/Kinkyuu_Has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6460</th>\n",
       "      <td>Monkey Turn</td>\n",
       "      <td>Based on Kawai Katsutoshi's manga of the same ...</td>\n",
       "      <td>https://myanimelist.net/anime/30757/Monster_St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17346</th>\n",
       "      <td>Ryman's Club</td>\n",
       "      <td>Ryman's Club is set in a badminton business gr...</td>\n",
       "      <td>https://myanimelist.net/anime/38006/Renmei_Kuu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9928</th>\n",
       "      <td>Chuumon no Ooi Ryouriten</td>\n",
       "      <td>The oldest anime adaption of the famous short ...</td>\n",
       "      <td>https://myanimelist.net/anime/43413/Chiyuki_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>Animegataris (Anime-Gataris)</td>\n",
       "      <td>After dreaming about an anime she used to watc...</td>\n",
       "      <td>https://myanimelist.net/anime/5849/Zettai_Kare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>Ganbare! Kakure Ase Project</td>\n",
       "      <td>An animated commercial for Kao Biore Sarasara ...</td>\n",
       "      <td>https://myanimelist.net/anime/19165/Fate_Zero_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15697</th>\n",
       "      <td>Ganbare! Lulu Lolo (TINY★TWIN★BEARS)</td>\n",
       "      <td>The new anime centers around the daily life of...</td>\n",
       "      <td>https://myanimelist.net/anime/23863/Ganbare_Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15698</th>\n",
       "      <td>Ganbare! Lulu Lolo 2nd Season (TINY★TWIN★BEARS...</td>\n",
       "      <td>The anime centers around the daily life of two...</td>\n",
       "      <td>https://myanimelist.net/anime/11593/Ganbare_Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15699</th>\n",
       "      <td>Ganbare! Lulu Lolo 3rd Season (TINY★TWIN★BEARS...</td>\n",
       "      <td>The anime centers around the daily life of two...</td>\n",
       "      <td>https://myanimelist.net/anime/39426/Ganbare_Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>Hi☆sCoool! SeHa Girls</td>\n",
       "      <td>The story of the anime will revolve around Dre...</td>\n",
       "      <td>https://myanimelist.net/anime/20391/Hidamari_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Shirobako</td>\n",
       "      <td>It all started in Kaminoyama High School, when...</td>\n",
       "      <td>https://myanimelist.net/anime/25835/Shirobako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6359</th>\n",
       "      <td>Pretty Rhythm: All Star Selection</td>\n",
       "      <td>This is the fourth anime series of Pretty Rhyt...</td>\n",
       "      <td>https://myanimelist.net/anime/36424/Princess_P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>Pokemon Best Wishes! Season 2: Decolora Advent...</td>\n",
       "      <td>Iris vs Ibuki! The Road to Become a Dragon Mas...</td>\n",
       "      <td>https://myanimelist.net/anime/15781/Puchimas__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13790</th>\n",
       "      <td>The Collected Animations of ICAF (2001-2006)</td>\n",
       "      <td>A collection of the best entries shown at the ...</td>\n",
       "      <td>https://myanimelist.net/anime/39359/Tezuka_Osa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>Boku wa Tomodachi ga Sukunai Episode 0</td>\n",
       "      <td>Hasegawa Kodaka has transferred schools, and h...</td>\n",
       "      <td>https://myanimelist.net/anime/21729/Cardfight_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14178</th>\n",
       "      <td>What's Michael? (TV)</td>\n",
       "      <td>Michael is a fat orange cat who lives with a p...</td>\n",
       "      <td>https://myanimelist.net/anime/38771/Wego_Graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>Jungle Taitei (1989)</td>\n",
       "      <td>Leo (a.k.a. Kimba in the U.S.) is a young ambi...</td>\n",
       "      <td>https://myanimelist.net/anime/2598/Juusenki_L-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>Ichigo 100%: Koi ga Hajimaru?! Satsuei Gasshuk...</td>\n",
       "      <td>Continues from where Ichigo 100% TV leaves off...</td>\n",
       "      <td>https://myanimelist.net/anime/10794/IS__Infini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>Oshiete! Galko-chan (Please tell me! Galko-chan)</td>\n",
       "      <td>At first glance, Galko, Otako, and Ojou are th...</td>\n",
       "      <td>https://myanimelist.net/anime/35752/Otsukimi_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9831</th>\n",
       "      <td>Ochou Fujin no Gensou</td>\n",
       "      <td>Butterfly, a faithful Japanese wife, waits pat...</td>\n",
       "      <td>https://myanimelist.net/anime/33228/Mobile_Sui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7786</th>\n",
       "      <td>Rusty Nail</td>\n",
       "      <td>Rusty Nail is a X Japan's anime music video pr...</td>\n",
       "      <td>https://myanimelist.net/anime/14935/Pokemon__U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15084</th>\n",
       "      <td>Byulbyul Iyagi 2</td>\n",
       "      <td>This film consists of 6 animated shorts produc...</td>\n",
       "      <td>https://myanimelist.net/anime/32237/Burutabu-chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>Flanders no Inu (A Dog of Flanders)</td>\n",
       "      <td>Nello Tarth is a poor but happy orphan who liv...</td>\n",
       "      <td>https://myanimelist.net/anime/23311/Garo__Hono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10608</th>\n",
       "      <td>Futago no Mahoutsukai Lico to Gli (The Wizard ...</td>\n",
       "      <td>Glico, the prominent food manufacturer best kn...</td>\n",
       "      <td>https://myanimelist.net/anime/36976/Cutie_Hone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9211</th>\n",
       "      <td>Himitsukessha Taka no Tsume Gaiden: Mukashi no...</td>\n",
       "      <td>The television shorts follow the original anim...</td>\n",
       "      <td>https://myanimelist.net/anime/5862/Fumoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15102</th>\n",
       "      <td>Cap Kakumei Bottleman (Cap Revolution Bottleman)</td>\n",
       "      <td>The anime centers around a boy named Kouta Kou...</td>\n",
       "      <td>https://myanimelist.net/anime/43509/Cao_Chong_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              animeTitle  \\\n",
       "15236  Choubakuretsu Ijigen Menko Battle: Gigant Shoo...   \n",
       "14725                             Ad Lib Anime Kenkyuujo   \n",
       "5510                   Nurse Witch Komugi-chan Magikarte   \n",
       "3846                                           Dr. Slump   \n",
       "16909                                   Kuakou de Qingwa   \n",
       "10388               Tamala 2010: A Punk Cat in Space OVA   \n",
       "13340                           Shounen H ga Mita Sensou   \n",
       "8607                                             Heybot!   \n",
       "14500                 Yowai Robot to 10 no Story Project   \n",
       "8359                                          Attraction   \n",
       "4268                             Macross Flash Back 2012   \n",
       "10157                                          Guilstein   \n",
       "6324                                      Kimagure Mercy   \n",
       "6460                                         Monkey Turn   \n",
       "17346                                       Ryman's Club   \n",
       "9928                            Chuumon no Ooi Ryouriten   \n",
       "6729                        Animegataris (Anime-Gataris)   \n",
       "8781                         Ganbare! Kakure Ase Project   \n",
       "15697               Ganbare! Lulu Lolo (TINY★TWIN★BEARS)   \n",
       "15698  Ganbare! Lulu Lolo 2nd Season (TINY★TWIN★BEARS...   \n",
       "15699  Ganbare! Lulu Lolo 3rd Season (TINY★TWIN★BEARS...   \n",
       "6100                               Hi☆sCoool! SeHa Girls   \n",
       "215                                            Shirobako   \n",
       "6359                   Pretty Rhythm: All Star Selection   \n",
       "5851   Pokemon Best Wishes! Season 2: Decolora Advent...   \n",
       "13790       The Collected Animations of ICAF (2001-2006)   \n",
       "4961              Boku wa Tomodachi ga Sukunai Episode 0   \n",
       "14178                               What's Michael? (TV)   \n",
       "5091                                Jungle Taitei (1989)   \n",
       "4327   Ichigo 100%: Koi ga Hajimaru?! Satsuei Gasshuk...   \n",
       "3432    Oshiete! Galko-chan (Please tell me! Galko-chan)   \n",
       "9831                               Ochou Fujin no Gensou   \n",
       "7786                                          Rusty Nail   \n",
       "15084                                   Byulbyul Iyagi 2   \n",
       "2031                 Flanders no Inu (A Dog of Flanders)   \n",
       "10608  Futago no Mahoutsukai Lico to Gli (The Wizard ...   \n",
       "9211   Himitsukessha Taka no Tsume Gaiden: Mukashi no...   \n",
       "15102   Cap Kakumei Bottleman (Cap Revolution Bottleman)   \n",
       "\n",
       "                                        animeDescription  \\\n",
       "15236  The anime follows Tsukasa, whose dream is to b...   \n",
       "14725  Emiri Katou and Kaori Fukuhara, the voice actr...   \n",
       "5510   Ungrar, the King of Viruses, has escaped from ...   \n",
       "3846   In Penguin Village, humans live alongside talk...   \n",
       "16909  The story is about a proud frog who likes to b...   \n",
       "10388  After a long 5-year wait, the team that create...   \n",
       "13340  An educational anime about the Pacific War. Ba...   \n",
       "8607   The anime takes place on the screw-shaped isla...   \n",
       "14500  The project is inspired by \"weak robot\", it is...   \n",
       "8359   Interactive anti-smoking anime. Directed by Ko...   \n",
       "4268   Flash Back 2012 is Minmei's farewell concert. ...   \n",
       "10157  The theatrical anime film is a full digital (3...   \n",
       "6324   Animated PV created for the release of Hachioj...   \n",
       "6460   Based on Kawai Katsutoshi's manga of the same ...   \n",
       "17346  Ryman's Club is set in a badminton business gr...   \n",
       "9928   The oldest anime adaption of the famous short ...   \n",
       "6729   After dreaming about an anime she used to watc...   \n",
       "8781   An animated commercial for Kao Biore Sarasara ...   \n",
       "15697  The new anime centers around the daily life of...   \n",
       "15698  The anime centers around the daily life of two...   \n",
       "15699  The anime centers around the daily life of two...   \n",
       "6100   The story of the anime will revolve around Dre...   \n",
       "215    It all started in Kaminoyama High School, when...   \n",
       "6359   This is the fourth anime series of Pretty Rhyt...   \n",
       "5851   Iris vs Ibuki! The Road to Become a Dragon Mas...   \n",
       "13790  A collection of the best entries shown at the ...   \n",
       "4961   Hasegawa Kodaka has transferred schools, and h...   \n",
       "14178  Michael is a fat orange cat who lives with a p...   \n",
       "5091   Leo (a.k.a. Kimba in the U.S.) is a young ambi...   \n",
       "4327   Continues from where Ichigo 100% TV leaves off...   \n",
       "3432   At first glance, Galko, Otako, and Ojou are th...   \n",
       "9831   Butterfly, a faithful Japanese wife, waits pat...   \n",
       "7786   Rusty Nail is a X Japan's anime music video pr...   \n",
       "15084  This film consists of 6 animated shorts produc...   \n",
       "2031   Nello Tarth is a poor but happy orphan who liv...   \n",
       "10608  Glico, the prominent food manufacturer best kn...   \n",
       "9211   The television shorts follow the original anim...   \n",
       "15102  The anime centers around a boy named Kouta Kou...   \n",
       "\n",
       "                                                animeUrl  \n",
       "15236  https://myanimelist.net/anime/42875/Chou_Futsu...  \n",
       "14725  https://myanimelist.net/anime/48586/ABC_Tenkiy...  \n",
       "5510   https://myanimelist.net/anime/1300/Omishi_Maho...  \n",
       "3846    https://myanimelist.net/anime/5197/Dragon_League  \n",
       "16909  https://myanimelist.net/anime/48011/Kuaile_Xia...  \n",
       "10388  https://myanimelist.net/anime/36162/Sou__Uchia...  \n",
       "13340  https://myanimelist.net/anime/8829/Shounen_Ash...  \n",
       "8607   https://myanimelist.net/anime/3906/H_P_Lovecra...  \n",
       "14500  https://myanimelist.net/anime/21129/Youtai_Nuh...  \n",
       "8359   https://myanimelist.net/anime/2537/Angelique__...  \n",
       "4268   https://myanimelist.net/anime/10370/Metal_Figh...  \n",
       "10157  https://myanimelist.net/anime/28077/Go_Go_575_...  \n",
       "6324   https://myanimelist.net/anime/4504/Kinkyuu_Has...  \n",
       "6460   https://myanimelist.net/anime/30757/Monster_St...  \n",
       "17346  https://myanimelist.net/anime/38006/Renmei_Kuu...  \n",
       "9928   https://myanimelist.net/anime/43413/Chiyuki_no...  \n",
       "6729   https://myanimelist.net/anime/5849/Zettai_Kare...  \n",
       "8781   https://myanimelist.net/anime/19165/Fate_Zero_...  \n",
       "15697  https://myanimelist.net/anime/23863/Ganbare_Sw...  \n",
       "15698  https://myanimelist.net/anime/11593/Ganbare_Bo...  \n",
       "15699  https://myanimelist.net/anime/39426/Ganbare_Ko...  \n",
       "6100   https://myanimelist.net/anime/20391/Hidamari_S...  \n",
       "215        https://myanimelist.net/anime/25835/Shirobako  \n",
       "6359   https://myanimelist.net/anime/36424/Princess_P...  \n",
       "5851   https://myanimelist.net/anime/15781/Puchimas__...  \n",
       "13790  https://myanimelist.net/anime/39359/Tezuka_Osa...  \n",
       "4961   https://myanimelist.net/anime/21729/Cardfight_...  \n",
       "14178  https://myanimelist.net/anime/38771/Wego_Graphics  \n",
       "5091   https://myanimelist.net/anime/2598/Juusenki_L-...  \n",
       "4327   https://myanimelist.net/anime/10794/IS__Infini...  \n",
       "3432   https://myanimelist.net/anime/35752/Otsukimi_R...  \n",
       "9831   https://myanimelist.net/anime/33228/Mobile_Sui...  \n",
       "7786   https://myanimelist.net/anime/14935/Pokemon__U...  \n",
       "15084  https://myanimelist.net/anime/32237/Burutabu-chan  \n",
       "2031   https://myanimelist.net/anime/23311/Garo__Hono...  \n",
       "10608  https://myanimelist.net/anime/36976/Cutie_Hone...  \n",
       "9211           https://myanimelist.net/anime/5862/Fumoon  \n",
       "15102  https://myanimelist.net/anime/43509/Cao_Chong_...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = parse_query(input().split(), vocab, stemmer)\n",
    "\n",
    "# if at least one word in the query\n",
    "# is in the vocabulary\n",
    "if query:\n",
    "    \n",
    "    results = get_results(query, inv_idx)\n",
    "\n",
    "    df_entries = get_df_entries(df, results)\n",
    "\n",
    "    if df_entries.size != 0: \n",
    "        display(df_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contains all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use é * Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "To solve this task, you will have to use the tfIdf score, and the Cosine similarity. The field to consider it is still the synopsis. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "```\n",
    "\n",
    "Practically, for each word you want the list of documents in which it is contained in, and the relative *tfIdf* score.\n",
    "\n",
    "__Tip__: *tfIdf* values are invariant with respect to the query, for this reason you can precalculate and store them accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_inv_idx(corpus, vocab):\n",
    "    tf = {}  \n",
    "    #creating nested dictionary tf[word_idx][doc_num] = tf = freq of the word in this document/ quantity of all words in the doc \n",
    "    for word in vocab:\n",
    "        tf[vocab[word]]={}     \n",
    "    for doc_num, doc in enumerate(corpus):\n",
    "        l = len(doc)\n",
    "        for word in doc:\n",
    "            if doc_num in tf[vocab[word]]:\n",
    "                tf[vocab[word]][doc_num] += 1/l #dividing by l - to obtain tf score\n",
    "            else:\n",
    "                tf[vocab[word]][doc_num] = 1/l   \n",
    "    idf = {i: np.log(len(corpus)/len(tf[i])) for i in tf}    #idf\n",
    "    #from nested dictionary to dict of tuples and tfidf = tf*idf\n",
    "    tfidf = {k: list((d, (t*(idf[k]))) for d,t in v.items()) for (k, v) in tf.items()}\n",
    "    return tfidf, idf \n",
    "\n",
    "inv_idx, idf = create_inv_idx(df['synopsis_clean'], vocab)\n",
    "save_dict_to_file(inv_idx, \"inv_idx_tfldf.json\")\n",
    "save_dict_to_file(idf, \"idf.json\") #let's save idf too, as they are invariant and we will need them for query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this new setting, given a query you get the right set of documents (i.e., those containing all the words in the query) and sort them according to their similairty to the query. For this purpose, as scoring function we will use the Cosine Similarity with respect to the *tfIdf* representations of the documents.\n",
    "\n",
    "Given a query, that you let the user enter:\n",
    "```\n",
    "saiyan race\n",
    "```\n",
    "the search engine is supposed to return a list of documents, __ranked__ by their Cosine Similarity with respect to the query entered in input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "* `animeTitle`\n",
    "* `animeDescription`\n",
    "* `Url`\n",
    "* The similarity score of the documents with respect to the query (float value between 0 and 1)\n",
    "\n",
    "\n",
    "__Example Output__:\n",
    "\n",
    "| animeTitle | animeDescription | Url | Similarity |\n",
    "|:-------------------------------------:|:-----:|:---------------------------------------------------------------------:|------------|\n",
    "| Fullmetal Alchemist: Brotherhood | ... | https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood | 0.96 |\n",
    "| Gintama | ... | https://myanimelist.net/anime/28977/Gintama%C2%B0 | 0.92 |\n",
    "| Shingeki no Kyojin Season 3 Part 2 | ... | https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2 | 0.87 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tfidf_query(corpus, query, inv_idx, idf):\n",
    "    \"\"\"\n",
    "    This functions receives query and outputs tfidfs for words in this query\n",
    "    \n",
    "    Arguments\n",
    "        corpus\n",
    "        query : list of words\n",
    "        inv_idx \n",
    "        idf\n",
    "    Returns \n",
    "        tfidf_q, tfidf_docs - tfidf for query and for documents,\n",
    "        tfidf_q - array, ith elemnt tfidf for ith word in query in query\n",
    "        tfidf_docs[\"doc_number\"] - array, ith elemnt tfidf for ith ford in query in doc_number\n",
    "        \n",
    "    \"\"\"\n",
    "    #tfidf indexes of words in query for all documents, in format of nested dict\n",
    "    tfidf_d = {q: dict((i, v) for (i, v) in inv_idx[q]) for q in query} \n",
    "    docs = set.intersection(*[set(i for (i, v) in inv_idx[q]) for q in query]) #list of docs that have all words from query\n",
    "    idf = {q: idf[q] for q in query} #idf for words from query\n",
    "    #let's count tfidf for query\n",
    "    l = len(query)\n",
    "    tf_q ={}\n",
    "    for word in query:\n",
    "        if word in tf_q:\n",
    "            tf_q[word] += 1/l #dividing by l - to obtain tf score\n",
    "        else:\n",
    "            tf_q[word] = 1/l   \n",
    "    tfidf_q = [tf_q[i]*idf[i] for i in tf_q]  #tfidf for query, array\n",
    "    \n",
    "    #creating tfidf for words in query for each doc that has all words from query\n",
    "    \n",
    "    tfidf_docs = {d : [0 for i in tf_q] for d in docs} #empty dict of arrays \n",
    "    for d in docs:\n",
    "        for i, w in enumerate(tf_q):\n",
    "            tfidf_docs[d][i] = tfidf_d[w][d]\n",
    "            \n",
    "    return tfidf_q, tfidf_docs\n",
    "\n",
    "def cosine_similiarity(q, d):\n",
    "    '''\n",
    "    This function counts cosine similiarity between two vectors q and d\n",
    "    '''\n",
    "    return (sum([q[i]*d[i] for i in range(len(q))]) / math.sqrt(sum(qi * qi for qi in q) * sum(di * di for di in d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best movie\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>animeUrl</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6384</th>\n",
       "      <td>Pokemon: Meloetta no Kirakira Recital</td>\n",
       "      <td>The 24th Pikachu short, set to premiere alongi...</td>\n",
       "      <td>https://myanimelist.net/anime/15961/Shakotan★B...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>Pokemon: Pikachu to Eievui Friends (Pokemon: E...</td>\n",
       "      <td>The 25th Pikachu short, set to premiere alongs...</td>\n",
       "      <td>https://myanimelist.net/anime/12223/sCRYed_Alt...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>Inazuma Eleven: Chou Jigen Dream Match</td>\n",
       "      <td>A new movie featuring a match between fan-vote...</td>\n",
       "      <td>https://myanimelist.net/anime/37210/Isekai_Mao...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3549</th>\n",
       "      <td>Tenjou Tenge: The Past Chapter (TenjhoTenge: T...</td>\n",
       "      <td>Mitsuomi Takayanagi and Maya Natsume both want...</td>\n",
       "      <td>https://myanimelist.net/anime/41353/The_God_of...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>Ichigo 100%: Koi ga Hajimaru?! Satsuei Gasshuk...</td>\n",
       "      <td>Continues from where Ichigo 100% TV leaves off...</td>\n",
       "      <td>https://myanimelist.net/anime/10794/IS__Infini...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15084</th>\n",
       "      <td>Byulbyul Iyagi 2</td>\n",
       "      <td>This film consists of 6 animated shorts produc...</td>\n",
       "      <td>https://myanimelist.net/anime/32237/Burutabu-chan</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12612</th>\n",
       "      <td>Oshare Majo Love and Berry: Shiawase no Mahou</td>\n",
       "      <td>A movie based on the popular fashion card and ...</td>\n",
       "      <td>https://myanimelist.net/anime/35175/Osakini_Do...</td>\n",
       "      <td>0.945966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8356</th>\n",
       "      <td>Memories Off #5 Togireta Film The Animation</td>\n",
       "      <td>Kawai Haruto belongs to a cinema club to reali...</td>\n",
       "      <td>https://myanimelist.net/anime/36728/Mameneko</td>\n",
       "      <td>0.886179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              animeTitle  \\\n",
       "6384               Pokemon: Meloetta no Kirakira Recital   \n",
       "4632   Pokemon: Pikachu to Eievui Friends (Pokemon: E...   \n",
       "4093              Inazuma Eleven: Chou Jigen Dream Match   \n",
       "3549   Tenjou Tenge: The Past Chapter (TenjhoTenge: T...   \n",
       "4327   Ichigo 100%: Koi ga Hajimaru?! Satsuei Gasshuk...   \n",
       "15084                                   Byulbyul Iyagi 2   \n",
       "12612      Oshare Majo Love and Berry: Shiawase no Mahou   \n",
       "8356         Memories Off #5 Togireta Film The Animation   \n",
       "\n",
       "                                        animeDescription  \\\n",
       "6384   The 24th Pikachu short, set to premiere alongi...   \n",
       "4632   The 25th Pikachu short, set to premiere alongs...   \n",
       "4093   A new movie featuring a match between fan-vote...   \n",
       "3549   Mitsuomi Takayanagi and Maya Natsume both want...   \n",
       "4327   Continues from where Ichigo 100% TV leaves off...   \n",
       "15084  This film consists of 6 animated shorts produc...   \n",
       "12612  A movie based on the popular fashion card and ...   \n",
       "8356   Kawai Haruto belongs to a cinema club to reali...   \n",
       "\n",
       "                                                animeUrl  Similarity  \n",
       "6384   https://myanimelist.net/anime/15961/Shakotan★B...    1.000000  \n",
       "4632   https://myanimelist.net/anime/12223/sCRYed_Alt...    1.000000  \n",
       "4093   https://myanimelist.net/anime/37210/Isekai_Mao...    1.000000  \n",
       "3549   https://myanimelist.net/anime/41353/The_God_of...    1.000000  \n",
       "4327   https://myanimelist.net/anime/10794/IS__Infini...    1.000000  \n",
       "15084  https://myanimelist.net/anime/32237/Burutabu-chan    1.000000  \n",
       "12612  https://myanimelist.net/anime/35175/Osakini_Do...    0.945966  \n",
       "8356        https://myanimelist.net/anime/36728/Mameneko    0.886179  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import heapq\n",
    "k = 10 #let's k be 10\n",
    "query = parse_query(input().split(), vocab)\n",
    "if query:\n",
    "    tfidf_q, tfidf_docs = tfidf_query(df['synopsis_clean'], query, inv_idx, idf)\n",
    "    #definig heap to finding k docs with largest cos similiarity\n",
    "    if len(tfidf_docs) < k:\n",
    "        larg_doc = {d: cosine_similiarity(tfidf_q, tfidf_docs[d]) for d in tfidf_docs.keys()}\n",
    "    else:\n",
    "        heap = [(cosine_similiarity(tfidf_q, tfidf_docs[key]), key) for key,value in tfidf_docs.items()]\n",
    "        larg_doc = {d: simil for (simil, d) in heapq.nlargest(k, heap)}\n",
    "    df_entries = get_df_entries(df, set(larg_doc.keys()), simil = larg_doc)\n",
    "    display(df_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEFINE A NEW SCORE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "You consult for a personal trainer who has a back-to-back sequence of requests for appointments. A sequence of requests is of the form > 30, 40, 25, 50, 30, 20 where each number is the time that the person who makes the appointment wants to spend. You need to accept some requests, however you need a break between them, so you cannot accept two consecutive requests. For example, [30, 50, 20] is an acceptable solution (of duration 100), but [30, 40, 50, 20] is not, because 30 and 40 are two consecutive appointments. Your goal is to provide to the personal trainer a schedule that maximizes the total length of the accepted appointments. For example, in the previous instance, the optimal solution is [40, 50, 20], of total duration 110.\n",
    "\n",
    "* Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "* Implement a program that given in input an instance in the form given above, gives the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoritmo (array):\n",
    "    a=0\n",
    "    b=array[0]\n",
    "    for elem in array[1:]:\n",
    "        n=max(a+elem,b)\n",
    "        a=b\n",
    "        b=n\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array=[3,4,5,60,4]\n",
    "algoritmo(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0e40e7c7a66e87c69eaa7498d7778a1d8fa6b3e422091d0b3e8dafd8f730247"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
