{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes\n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "The main idea is to split the work between the retrieving of the html pages (skipping the already stored urls) and the retrieving of the links in the pages.<br>\n",
    "Once we've defined a way to scrap the urls from the page, we'll iterate over 20000 to get all the pages till that number. At the time of execution the urls retrieved was only 19122, but in other execution it was even 19128, so it depends from the moment you execute that. The file was created in 5-Nov-2021 so the list refers to the content of the site in that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "url = 'https://myanimelist.net/topanime.php'\n",
    "response= requests.get(url)\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_soup(soup):\n",
    "    ''' \n",
    "        Given a BeautifulSoup object the functions iterates over all the links in the table rows in the corrisponding page.\n",
    "        The functions also returns the list containing all the destination (href) of these links\n",
    "    '''\n",
    "    anime = []\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:\n",
    "            # checking if there is a content in the link\n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime.append((link.contents[0], link.get('href')))\n",
    "    return anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_list= []\n",
    "for lim in range(0, 20000, 50):\n",
    "    \n",
    "    # The principal page of animelis, so: https://myanimelist.net/topanime.php\n",
    "    if lim==0:\n",
    "        new_url = url\n",
    "    else: # we've to skio the first lim elements\n",
    "        new_url = url+'?limit='+str(lim)\n",
    "    response = requests.get(new_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tot_list += get_links_from_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that the list contains couples in the form: name, link and we want only the link, we save only the second element\n",
    "\n",
    "with open('out.txt', 'w') as f:\n",
    "    for n, link in tot_list:\n",
    "        f.write(link+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "* Download the html corresponding to each of the collected urls.\n",
    "* After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the * * * stopping point. More details in Important (2).\n",
    "* Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Due to the large amount of pages you need to download, follow the next tips that will help you speeding up several time-consuming operations.\n",
    "\n",
    "[Save time downloading files] You are asked to crawl a considerable number of pages, which will take plenty of time. To speed up the operation, we suggest you to work in parallel with your group's colleagues or even generate code that works in parallel with all the CPUs available in your computer. In particular, using the same code, each component of the group can be in charge of downloading a subset of pages (e.g., the first 100). PAY ATTENTION: Once obtained all the pages, merge your results into an unique dataset. In fact, the search engine must look up for results in the whole set of documents.\n",
    "\n",
    "[Save your data] It is not nice to restart a crawling procedure, given its runtime. For this reason, it is extremely important that for every time you crawl a page, you must save it with the name article_i.html, where i corresponds to the number of articles you have already downloaded. In such way, if something goes bad, you can restart your crawling procedure from the i+1-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "Once the website has to protect itself, it will block our requests if they are too much, so i've looked first if there's an error and, in that case, the program stops for a random range of time.<br>\n",
    "In this way we've collected only cleaned pages (not the one containing the error button).<br>\n",
    "The program (that can be found in the [downloder.py](downloader.py) file) has not been run by this notebook, but we've made a CLI version in order to run it from the terminal in how many terminal you want in order to work in parallel.<br>\n",
    "The specification of that function can be retrieved in that file and even launching it from terminal with the ```--h``` flag only to see the documentation and the usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "* **Anime Name** (to save as animeTitle): *String*\n",
    "* **Anime Type** (to save as animeType): *String*\n",
    "* **Number of episode** (to save as animeNumEpisode): *Integer*\n",
    "* **Release and End Dates of anime** (to save as releaseDate and endDate): Convert both release and end date into *datetime format*.\n",
    "* **Number of members** (to save as animeNumMembers): *Integer*\n",
    "* **Score** (to save as animeScore): *Float*\n",
    "* **Users** (to save as animeUsers): *Integer*\n",
    "* **Rank** (to save as animeRank): *Integer*\n",
    "* **Popularity** (to save as animePopularity): *Integer*\n",
    "* **Synopsis** (to save as animeDescription): *String*\n",
    "* **Related Anime** (to save as animeRelated): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. *List of strings*.\n",
    "* **Characters** (to save as animeCharacters): *List of strings*.\n",
    "* **Voices** (to save as animeVoices): *List of strings*\n",
    "* **Staff** (to save as animeStaff): Include the staff name and their responsibility/task in a *list of lists*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "All the needed functions are in the [html_parser.py](./html_parser.py) file that contains the relative comments and documentation too.\n",
    "The only function that has to be called to entirely solve this point is the ```save_tsv_info``` function.<br>\n",
    "Even if the function that will be called in order to get all the needed data in the tsv format is only one, the function implemented are several and more or less they are grouped in three category:\n",
    "* support function (to obtain a BeautifulSoup object from the html o to retrieve the datetime format from a string)\n",
    "* webscraping function: They are the core of this implementation, all of them takes as parameter a BeautifulSoup object and a dictionary and their purpouse is to put into this dictionary as much information as possible starting from the object in input\n",
    "* applicative function: They allow to directly obtain the data from a file o an index employing the other functions (i.e. ```save_tsv_info```)\n",
    "\n",
    "### Webscraping:\n",
    "In order to retrieve the 13 required fields we need first of all to study the structure of the page.<br>\n",
    "In general the title is simple to retrieve using bs.<br>\n",
    "The page, in general is structured as a series of nested tables, so we have a huge table with two columns, one for the left menu that contains all the info relatives to the score, the type of the serie, the ranking and so on and the other is the real body of the page, containing the synopsis, the related animes and other required fields.<br>\n",
    "<newline>\n",
    "Considering that, we have written a function called ```get_left_attributes``` that retrieves the values for the fields:\n",
    "* episodes\n",
    "* start_date\n",
    "* end_date\n",
    "* score\n",
    "* users\n",
    "* rank\n",
    "* members\n",
    "* popularity\n",
    "* type\n",
    "These fields are all in one div for each one and this div has the class equal to '*spaceit_pad*', then we have as content of this div a list of values in the form (at exception of the score) ```['\\n', name_of_the_value, value]``` then value has to be processed in order to extract the date or the integer and so on.<br>\n",
    "<newline>\n",
    "\n",
    "The remaining values can be found in the right column of the huge table inside different divs or subtables.<br>\n",
    "<newline>\n",
    "\n",
    "#### Synopsis<br>\n",
    "Can be found simply in a paragraph with the attribute '*itemprop*' equal to 'description'<br>\n",
    "#### Related animes<br>\n",
    "They are inside a dedicated table with class '*anime_detail_related_anime*', in particular we want only the animes for which exists a link, so we'll iterate over all the links in that table and we'll take the different contents returning the unique values\n",
    "#### Staff<br>\n",
    "In this case (as for the next) the function will work directly on a specific div (the divs that has the class '*detail-characters-list clearfix*')<br>\n",
    "Given that div we have all the names that we need as content of links so we'll simply iterate over it and collect the data of interest\n",
    "#### Characters and voices<br>\n",
    "This div has the same class of the above one, so if we have only one of the two we can distinguish them by the content of an h3 element of the class '*h3_characters_voice_actors*'.<br>\n",
    "Starting from this div we have other two div in order to split the list in two columns, these divs contains a list of tables, each of them contains only one row and three columns, one for the images of the characters, one for the name of the characters and the third for the voice, so we'll iterate over the row elements and selecting only the second two columns.<br>\n",
    "From the first and the second we'll get the link element containing the name either of the character or for the voice.\n",
    "#### Putting all togheter<br>\n",
    "It's done with a function that uses each of these subfunction in order to retrieve, given an html page, the content of interest in a dictionary\n",
    "<newline>\n",
    "\n",
    "Once we can obtain a dictionary with the data of interest converting and saving it in a tsv file is trivial and it's done by ad-hoc functions. All this work is encapsulated in the funciton ```save_tsv_info``` that takes the ranges of the page for which we that we want to save the scraped tsv, the base directory where these page are stored and the destination directory over which the outputs will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Fullmetal Alchemist: Brotherhood',\n",
       " 'type': 'TV',\n",
       " 'episodes': 64,\n",
       " 'start_date': datetime.datetime(2009, 4, 5, 0, 0),\n",
       " 'end_date': datetime.datetime(2010, 7, 4, 0, 0),\n",
       " 'score': 9.16,\n",
       " 'users': 1622384,\n",
       " 'ranked': 1,\n",
       " 'popularity': 3,\n",
       " 'members': 2675906,\n",
       " 'synopsis': \"After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.\",\n",
       " 'related_anime': ['Fullmetal Alchemist: Brotherhood - 4-Koma Theater',\n",
       "  'Fullmetal Alchemist: Brotherhood Specials',\n",
       "  'Fullmetal Alchemist',\n",
       "  'Fullmetal Alchemist: The Sacred Star of Milos'],\n",
       " 'characters': ['Elric, Edward',\n",
       "  'Elric, Alphonse',\n",
       "  'Mustang, Roy',\n",
       "  'Hughes, Maes',\n",
       "  'Greed',\n",
       "  'Hawkeye, Riza',\n",
       "  'Yao, Ling',\n",
       "  'Armstrong, Alex Louis',\n",
       "  'Rockbell, Winry',\n",
       "  'Armstrong, Olivier Mira'],\n",
       " 'voices': ['Park, Romi',\n",
       "  'Kugimiya, Rie',\n",
       "  'Miki, Shinichiro',\n",
       "  'Fujiwara, Keiji',\n",
       "  'Nakamura, Yuuichi',\n",
       "  'Orikasa, Fumiko',\n",
       "  'Miyano, Mamoru',\n",
       "  'Utsumi, Kenji',\n",
       "  'Takamoto, Megumi',\n",
       "  'Soumi, Youko'],\n",
       " 'staff': [['Cook, Justin', ['Producer']],\n",
       "  ['Yonai, Noritomo', ['Producer']],\n",
       "  ['Irie, Yasuhiro', ['Director', ' Episode Director', ' Storyboard']],\n",
       "  ['Mima, Masafumi', ['Sound Director']]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_parser.get_total_info('../data/html_pages/article_00000.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each anime, you create an anime_i.tsv file of this structure:\n",
    "\n",
    "animeTitle \\t animeType \\t  ... \\t animeStaff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "In this case we've simply runned the function ```save_tsv_info``` starting from 0 to 19123 (the total number of the pages) in order to get all the necessary.\n",
    "An example of the output of the funcion that gets the tsv is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The header of the tsv table is the following:\n",
      "\ttitle\ttype\tepisodes\tstart_date\tend_date\tscore\tusers\tranked\tpopularity\tmembers\tsynopsis\trelated_anime\tcharacters\tvoices\tstaff\n",
      "\n",
      "The content for the 0-th page (the one of the dictionary above) is the following:\n",
      "\tFullmetal Alchemist: Brotherhood\tTV\t64\t2009-04-05 00:00:00\t2010-07-04 00:00:00\t9.16\t1622384\t1\t3\t2675906\tAfter a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.\t['Fullmetal Alchemist: Brotherhood - 4-Koma Theater', 'Fullmetal Alchemist: Brotherhood Specials', 'Fullmetal Alchemist', 'Fullmetal Alchemist: The Sacred Star of Milos']\t['Elric, Edward', 'Elric, Alphonse', 'Mustang, Roy', 'Hughes, Maes', 'Greed', 'Hawkeye, Riza', 'Yao, Ling', 'Armstrong, Alex Louis', 'Rockbell, Winry', 'Armstrong, Olivier Mira']\t['Park, Romi', 'Kugimiya, Rie', 'Miki, Shinichiro', 'Fujiwara, Keiji', 'Nakamura, Yuuichi', 'Orikasa, Fumiko', 'Miyano, Mamoru', 'Utsumi, Kenji', 'Takamoto, Megumi', 'Soumi, Youko']\t[['Cook, Justin', ['Producer']], ['Yonai, Noritomo', ['Producer']], ['Irie, Yasuhiro', ['Director', ' Episode Director', ' Storyboard']], ['Mima, Masafumi', ['Sound Director']]]\n"
     ]
    }
   ],
   "source": [
    "header, content = html_parser.get_tsv_from_idx(0)\n",
    "print(f\"The header of the tsv table is the following:\\n\\t{header}\")\n",
    "print()\n",
    "print(f\"The content for the 0-th page (the one of the dictionary above) is the following:\\n\\t{content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the TSVs!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line is commented for safety reason, the execution will bring to an error given that the data doesn't exist in your machine\n",
    "\n",
    "#html_parser.save_tsv_info(0, 1923) # We've used the default directory for source and dest, see the code for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SEARCH ENGINE\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the animes that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each anime by:\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Stemming\n",
    "* Anything else you think it's needed\n",
    "\n",
    "For this purpose, you can use the nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!\n",
    "Before building the index,\n",
    "\n",
    "Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "where document_i is the id of a document that contains the word.\n",
    "\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth to think to store it in a separate file and load it in memory when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "saiyan race\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "animeTitle\n",
    "animeDescription\n",
    "Url\n",
    "Example Output:\n",
    "\n",
    "animeTitle\tanimeDescription\tUrl\n",
    "Fullmetal Alchemist: Brotherhood\t...\thttps://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n",
    "Gintama\t...\thttps://myanimelist.net/anime/28977/Gintama%C2%B0\n",
    "Shingeki no Kyojin Season 3 Part 2\t...\thttps://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2\n",
    "If everything works well in this step, you can go to the next point, and make your Search Engine more complex and better in answering queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contains all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use Ã© * Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "To solve this task, you will have to use the tfIdf score, and the Cosine similarity. The field to consider it is still the synopsis. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEFINE A NEW SCORE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "You consult for a personal trainer who has a back-to-back sequence of requests for appointments. A sequence of requests is of the form > 30, 40, 25, 50, 30, 20 where each number is the time that the person who makes the appointment wants to spend. You need to accept some requests, however you need a break between them, so you cannot accept two consecutive requests. For example, [30, 50, 20] is an acceptable solution (of duration 100), but [30, 40, 50, 20] is not, because 30 and 40 are two consecutive appointments. Your goal is to provide to the personal trainer a schedule that maximizes the total length of the accepted appointments. For example, in the previous instance, the optimal solution is [40, 50, 20], of total duration 110.\n",
    "\n",
    "* Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "* Implement a program that given in input an instance in the form given above, gives the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoritmo (array):\n",
    "    a=0\n",
    "    b=array[0]\n",
    "    for elem in array[1:]:\n",
    "        n=max(a+elem,b)\n",
    "        a=b\n",
    "        b=n\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array=[3,4,5,60,4]\n",
    "algoritmo(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "caso_limite = [50, 65, 90, 60, 35, 10, 15, 25]\n",
    "print(algoritmo(caso_limite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 90, 35, 25]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recursive_algo(arr):\n",
    "    if len(arr)<2:\n",
    "        return arr\n",
    "\n",
    "    return(max(recursive_algo(arr[1:]), [arr[0]] + recursive_algo(arr[2:]), key=sum))\n",
    "\n",
    "recursive_algo(caso_limite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 massimo fra 65 50 : n= 65 a= 0 b= 50\n",
      "90 massimo fra 140 65 : n= 140 a= 50 b= 65\n",
      "60 massimo fra 125 140 : n= 140 a= 65 b= 140\n",
      "35 massimo fra 175 140 : n= 175 a= 140 b= 140\n",
      "10 massimo fra 150 175 : n= 175 a= 140 b= 175\n",
      "15 massimo fra 190 175 : n= 190 a= 175 b= 175\n",
      "25 massimo fra 200 190 : n= 200 a= 175 b= 190\n",
      "[50, 65, 90, 35, 15, 25]\n"
     ]
    }
   ],
   "source": [
    "def algoritmo_leo(array):\n",
    "    a=0\n",
    "    b=array[0]\n",
    "    step=[array[0]]\n",
    "    for elem in array[1:]:\n",
    "        n=max(a+elem,b)\n",
    "        if a+elem>b:\n",
    "            step.append(elem)\n",
    "        print(elem,'massimo fra',a+elem,b,': n=',n,'a=',a,'b=',b)\n",
    "        a=b\n",
    "        b=n\n",
    "    return b,step\n",
    "\n",
    "print(algoritmo_leo(caso_limite)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0e40e7c7a66e87c69eaa7498d7778a1d8fa6b3e422091d0b3e8dafd8f730247"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('fds': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
