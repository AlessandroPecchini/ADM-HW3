{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of animes\n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "url = 'https://myanimelist.net/topanime.php'\n",
    "response= requests.get(url)\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup.find_all('a') #Finds all the links\n",
    "#soup.find_all('tr') # Finds all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_soup(soup):\n",
    "    anime = []\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:\n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                anime.append((link.contents[0], link.get('href')))\n",
    "    return anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_list= []\n",
    "for lim in range(0, 20000, 50):\n",
    "    if lim==0:\n",
    "        new_url = url\n",
    "    else:\n",
    "        new_url = url+'?limit='+str(lim)\n",
    "    response = requests.get(new_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tot_list += get_links_from_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'w') as f:\n",
    "    for n, link in tot_list:\n",
    "        f.write(link+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl animes\n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "* Download the html corresponding to each of the collected urls.\n",
    "* After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the * * * stopping point. More details in Important (2).\n",
    "* Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out.txt\") as file:\n",
    "    #the line will be the url\n",
    "    number=0\n",
    "    \n",
    "    for url in file:\n",
    "        respone=requests.get(url)\n",
    "        soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "        soup=str(soup)\n",
    "        #save file as htlm\n",
    "        name=('article_%d.html' % (number))\n",
    "        html_file = open(name,\"w\")\n",
    "        html_file.write(soup)\n",
    "        html_file.close()\n",
    "        number+=1\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Due to the large amount of pages you need to download, follow the next tips that will help you speeding up several time-consuming operations.\n",
    "\n",
    "[Save time downloading files] You are asked to crawl a considerable number of pages, which will take plenty of time. To speed up the operation, we suggest you to work in parallel with your group's colleagues or even generate code that works in parallel with all the CPUs available in your computer. In particular, using the same code, each component of the group can be in charge of downloading a subset of pages (e.g., the first 100). PAY ATTENTION: Once obtained all the pages, merge your results into an unique dataset. In fact, the search engine must look up for results in the whole set of documents.\n",
    "\n",
    "[Save your data] It is not nice to restart a crawling procedure, given its runtime. For this reason, it is extremely important that for every time you crawl a page, you must save it with the name article_i.html, where i corresponds to the number of articles you have already downloaded. In such way, if something goes bad, you can restart your crawling procedure from the i+1-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "* **Anime Name** (to save as animeTitle): *String*\n",
    "* **Anime Type** (to save as animeType): *String*\n",
    "* **Number of episode** (to save as animeNumEpisode): *Integer*\n",
    "* **Release and End Dates of anime** (to save as releaseDate and endDate): Convert both release and end date into *datetime format*.\n",
    "* **Number of members** (to save as animeNumMembers): *Integer*\n",
    "* **Score** (to save as animeScore): *Float*\n",
    "* **Users** (to save as animeUsers): *Integer*\n",
    "* **Rank** (to save as animeRank): *Integer*\n",
    "* **Popularity** (to save as animePopularity): *Integer*\n",
    "* **Synopsis** (to save as animeDescription): *String*\n",
    "* **Related Anime** (to save as animeRelated): Extract all the related animes, but only keep unique values and those that have a hyperlink associated to them. *List of strings*.\n",
    "* **Characters** (to save as animeCharacters): *List of strings*.\n",
    "* **Voices** (to save as animeVoices): *List of strings*\n",
    "* **Staff** (to save as animeStaff): Include the staff name and their responsibility/task in a *list of lists*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each anime, you create an anime_i.tsv file of this structure:\n",
    "\n",
    "animeTitle \\t animeType \\t  ... \\t animeStaff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SEARCH ENGINE\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the animes that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each anime by:\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Stemming\n",
    "* Anything else you think it's needed\n",
    "\n",
    "For this purpose, you can use the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"../shared_stuff/tsv_files/total_pages.tsv\",\n",
    "                       delimiter = \"\\t\",\n",
    "                       header = \"infer\",\n",
    "                       on_bad_lines = \"skip\") # fix line 16473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s):\n",
    "    \"\"\"\n",
    "    This functions returns a list\n",
    "    with each of the characters of the input string\n",
    "    \n",
    "    Arguments\n",
    "        s : string\n",
    "        \n",
    "    Returns\n",
    "        (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    return [char for char in s]\n",
    "\n",
    "def has_digits(s):\n",
    "    \"\"\"\n",
    "    This function checks whether a string\n",
    "    contains any digits\n",
    "    \n",
    "    Arguments\n",
    "        s : string\n",
    "        \n",
    "    Returns\n",
    "        (bool) True / False\n",
    "    \"\"\"\n",
    "    \n",
    "    return len([char for char in s if char.isdigit()]) != 0\n",
    "\n",
    "def bad_words():\n",
    "    \"\"\"\n",
    "    This function creates a list with words\n",
    "    that should be excluded from the vocabulary\n",
    "    during preprocessing, including punctuation,\n",
    "    stopwords et similia\n",
    "    \n",
    "    Arguments\n",
    "        none\n",
    "        \n",
    "    Returns\n",
    "        (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    punct = str_to_list(string.punctuation)\n",
    "    punct += [\"...\", \"''\", \"``\", '\"\"']\n",
    "    \n",
    "    stops = stopwords.words(\"english\")\n",
    "    \n",
    "    other_suffixes = [\"'s\", \"n't\"]\n",
    "    \n",
    "    return punct + stops + other_suffixes\n",
    "\n",
    "def preprocess(text, stemmer):\n",
    "    \"\"\"\n",
    "    This function preprocesses some text (a document)\n",
    "    by isolating each word, excluding stopwords et similia,\n",
    "    and finally stemming them\n",
    "    \n",
    "    Arguments\n",
    "        text : (string)\n",
    "        stemmer : stemmer object, e.g. SnowBallStemmer()\n",
    "    \n",
    "    Returns\n",
    "        (list) preprocessed input text\n",
    "    \"\"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "        \n",
    "    return [stemmer.stem(w) for w in tokens \n",
    "            if w not in bad_words() and not has_digits(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "df['synopsis_clean'] = df.apply(lambda row: preprocess(row['synopsis'], stemmer), \n",
    "                                axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!\n",
    "Before building the index,\n",
    "\n",
    "Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "where document_i is the id of a document that contains the word.\n",
    "\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth to think to store it in a separate file and load it in memory when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(corpus):\n",
    "    \"\"\"\n",
    "    This function creates a set of unique\n",
    "    and preprocessed words from a corpus\n",
    "    \n",
    "    Arguments\n",
    "        corpus : pandas df column or list-like\n",
    "    \n",
    "    Returns\n",
    "        vocab  : dictionary with the words as keys\n",
    "                 and a unique integer for each as values\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()\n",
    "    \n",
    "    for doc in corpus:\n",
    "        vocab.update(set(doc))\n",
    "\n",
    "    return {word:idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "\n",
    "def save_dict_to_file(dct, filename):\n",
    "    \"\"\"\n",
    "    This function saves a dictionary \n",
    "    to an external JSON file\n",
    "    \n",
    "    Arguments\n",
    "        dct       : dictionary\n",
    "        filename  : name of the file\n",
    "        \n",
    "    Returns\n",
    "        void\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(dct, file)\n",
    "        \n",
    "\n",
    "def read_dict_from_file(filename):\n",
    "    \"\"\"\n",
    "    This function reads a dictionary\n",
    "    from an external JSON file\n",
    "        \n",
    "    Arguments\n",
    "        filename : name of the file\n",
    "    \n",
    "    Returns\n",
    "        dct : dictionary with the contents of 'filename'\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        dct = json.loads(file.read())\n",
    "\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute this cell the first time or \n",
    "# when the preprocessing changes!\n",
    "\n",
    "vocab = create_vocab(df['synopsis_clean'])\n",
    "\n",
    "save_dict_to_file(vocab, \"vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = read_dict_from_file(\"vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inv_idx(corpus, vocab): \n",
    "    \"\"\"\n",
    "    This functions creates an inverted index list\n",
    "    given a corpus of documents and a vocabulary\n",
    "    \n",
    "    Arguments\n",
    "        corpus  : pandas df column or list-like\n",
    "        vocab   : dictionary of all the words in the corpus\n",
    "    \n",
    "    Returns\n",
    "        inv_idx : dictionary with the words as referenced in 'vocab' as keys \n",
    "                  and the lists of the documents each word is in as values       \n",
    "    \"\"\"\n",
    "    \n",
    "    inv_idx = {}\n",
    "    \n",
    "    for idx, word in zip(vocab.values(), vocab.keys()):\n",
    "        inv_idx[idx] = [doc_id for doc_id, doc in enumerate(corpus) if word in doc]\n",
    "    \n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only execute this cell the first time or \n",
    "# when the preprocessing/vocabulary change!\n",
    "\n",
    "inv_idx = create_inv_idx(df['synopsis_clean'], vocab)\n",
    "\n",
    "#save_dict_to_file(inv_idx, \"inv_idx.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keys of the dict we get are str, not int like when we created it\n",
    "# I don't think it's necessary, but should we parse them when we read the json?\n",
    "inv_idx = read_dict_from_file(\"inv_idx.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query\n",
    "Given a query, that you let the user enter:\n",
    "\n",
    "saiyan race\n",
    "the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "animeTitle\n",
    "animeDescription\n",
    "Url\n",
    "Example Output:\n",
    "\n",
    "animeTitle\tanimeDescription\tUrl\n",
    "Fullmetal Alchemist: Brotherhood\t...\thttps://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n",
    "Gintama\t...\thttps://myanimelist.net/anime/28977/Gintama%C2%B0\n",
    "Shingeki no Kyojin Season 3 Part 2\t...\thttps://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2\n",
    "If everything works well in this step, you can go to the next point, and make your Search Engine more complex and better in answering queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query, vocab):\n",
    "    \"\"\"\n",
    "    This functions converts the list of words\n",
    "    input by the user into the list of the IDs\n",
    "    the words are saved as in the vocabulary\n",
    "    \n",
    "    Arguments\n",
    "        query : list of words\n",
    "        vocab : vocabulary of words with the words as keys\n",
    "                and their IDs as values\n",
    "    Returns\n",
    "        list of the IDs of the words in the query\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_query = []\n",
    "    \n",
    "    for word in query:\n",
    "        try:\n",
    "            parsed_query.append(vocab[stemmer.stem(word)])\n",
    "        except KeyError:\n",
    "            print(f\"The term '{word}' wasn't found anywhere!\")\n",
    "    \n",
    "    return parsed_query\n",
    "\n",
    "\n",
    "def get_results(query, inv_idx):\n",
    "    \"\"\"\n",
    "    This functions finds the documents all the words\n",
    "    in the query are in.\n",
    "    \n",
    "    It finds them in three steps:\n",
    "    1. creates a list of docs each word is in from the inverted index\n",
    "    2. converts that list into a set\n",
    "    3. intersects all those sets into a single set\n",
    "       \n",
    "    Arguments\n",
    "        query : list of words as parsed by 'parse_query'\n",
    "        \n",
    "    Returns\n",
    "        set with the documents that contain all the words in the query\n",
    "    \"\"\"\n",
    "    \n",
    "    return set.intersection(*[set(inv_idx[str(q)]) for q in query])\n",
    "\n",
    "\n",
    "def get_df_entries(df, results,\n",
    "                   url_file = \"../shared_stuff/url_list.txt\"):\n",
    "    \"\"\"\n",
    "    This function filters the dataset so it only shows\n",
    "    the rows which match the results, and adds a new column\n",
    "    with the URL for the anime of each row.\n",
    "    \n",
    "    Arguments\n",
    "        df       : pandas dataframe\n",
    "        results  : set with the row indices to be filtered out\n",
    "        url_file : external file with the URLs for each of the rows in df\n",
    "    \n",
    "    Returns\n",
    "        df : filtered pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results!\")\n",
    "        return\n",
    "    \n",
    "    with open(url_file, 'r') as file:\n",
    "        url_list = file.read().split(\"\\n\")\n",
    "\n",
    "    df = df.iloc[[*results]]\n",
    "    df = df[[\"title\", \"synopsis\"]]\n",
    "    df = df.rename(columns = {\"title\": \"animeTitle\", \n",
    "                              \"synopsis\": \"animeDescription\"})\n",
    "    \n",
    "    df['animeUrl'] = itemgetter(*results)(url_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " one piece goku\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results!\n"
     ]
    }
   ],
   "source": [
    "query = parse_query(input().split(), vocab)\n",
    "\n",
    "# if at least one word in the query\n",
    "# is in the vocabulary\n",
    "if query:\n",
    "    \n",
    "    results = get_results(query)\n",
    "\n",
    "    df_entries = get_df_entries(df, results)\n",
    "\n",
    "    if df_entries: \n",
    "        display(df_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contains all the words in the query.\n",
    "* Sort them by their similarity with the query.\n",
    "* Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use é * Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "To solve this task, you will have to use the tfIdf score, and the Cosine similarity. The field to consider it is still the synopsis. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bozza\n",
    "\n",
    "# def create_inv_idx(corpus, vocab):\n",
    "#     \n",
    "#     inv_idx = {}\n",
    "#     \n",
    "#     for doc_num, doc in enumerate(corpus):\n",
    "#         cnt = Counter(doc)\n",
    "#         for word, idx in zip(vocab.keys(), vocab.values()):\n",
    "#             inv_idx[idx] = (cnt[word])\n",
    "#     \n",
    "#     return inv_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEFINE A NEW SCORE!\n",
    "\n",
    "Now it's your turn. Build a new metric to rank animes based on the queries of their users.\n",
    "\n",
    "In this scenario, a single user can give in input more information than the single textual query, so you need to take into account all this information, and think a creative and logical way on how to answer at user's requests.\n",
    "\n",
    "#### Practically:\n",
    "\n",
    "The user will enter you a text query. As a starting point, get the query-related documents by exploiting the search engine of Step 2.1.<br>\n",
    "<newline>\n",
    "\n",
    "Once you have the documents, you need to sort them according to your new score. In this step you won't have anymore to take into account just the plot of the documents, you must use the remaining variables in your dataset (or new possible variables that you can create from the existing ones...). You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.<br>\n",
    "<newline>\n",
    "\n",
    "**Q: How to sort them?** <br>\n",
    "**A: Allow the user to specify more information** that you find in the documents, and define a new metric that ranks the results based on the new request. You can also use other information regarding the anime to score some animes above others.<br>\n",
    "N.B.: You have to define a scoring function, not a filter!\n",
    "\n",
    "The output, must contain:\n",
    "\n",
    "* **animeTitle**\n",
    "* **animeDescription**\n",
    "* **Url**\n",
    "* **The new similarity score of the documents** with respect to the query\n",
    "\n",
    "\n",
    "Are the results you obtain better than with the previous scoring function. Explain and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Solution\n",
    "\n",
    "The basic idea is that a user can search for the words in the synopsis in the 2.1 point, so here we want to give the possibility to the user to query the data in the points he preferes from:\n",
    "* title\n",
    "* staff\n",
    "* characters\n",
    "* voices\n",
    "* synopsis\n",
    "\n",
    "Given that the query shoudld be in the form:<br>\n",
    "\"```word1 word2 [where_to_search] word3 word4 [where_t_s2] ...```\"<br>\n",
    "and the program will return the documents that contains word1 AND word2 in the field 'where_to_search' AND contains word3 AND word4 in the field 'where_t_s2'\n",
    "<newline>\n",
    "\n",
    "Hence the idea is to create an inverted index for each of thie fields and then parse the query and obtain the document that match it.<br>\n",
    "We need to sort this document accordingly with a customized score and return for each of them the neccessarly info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing the data...\n",
    "Given that the file can contain data that are not so clean, we have to use some special attributes of the read_table function in order to retrieve a dataframe containing all the information of interest.<br>\n",
    "As expected. the dataframe contains exactly 19122 rows indexed from 0 to 19121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def import_df(path = \"../shared_stuff/tsv_files/0total_pages.tsv\"):\n",
    "    return pd.read_table(path,\n",
    "                        delimiter = \"\\t\",\n",
    "                        header = \"infer\",quoting=csv.QUOTE_NONE, error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19122 entries, 0 to 19121\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   title          19122 non-null  object\n",
      " 1   type           19122 non-null  object\n",
      " 2   episodes       19122 non-null  object\n",
      " 3   start_date     19122 non-null  object\n",
      " 4   end_date       19122 non-null  object\n",
      " 5   score          19122 non-null  object\n",
      " 6   users          19122 non-null  object\n",
      " 7   ranked         19122 non-null  object\n",
      " 8   popularity     19122 non-null  int64 \n",
      " 9   members        19122 non-null  int64 \n",
      " 10  synopsis       19122 non-null  object\n",
      " 11  related_anime  19122 non-null  object\n",
      " 12  characters     19122 non-null  object\n",
      " 13  voices         19122 non-null  object\n",
      " 14  staff          19122 non-null  object\n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 2.2+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-06228d5a45ed>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  df = import_df()\n"
     ]
    }
   ],
   "source": [
    "df = import_df()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '../shared_stuff/indexes/title',\n",
       " 'voices': '../shared_stuff/indexes/voices',\n",
       " 'staff': '../shared_stuff/indexes/staff',\n",
       " 'synopsis': '../shared_stuff/indexes/synopsis',\n",
       " 'characters': '../shared_stuff/indexes/characters'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def actual_indexes(path='../shared_stuff/indexes'):\n",
    "    ''' \n",
    "        Given the path containing the stored indexes the function returns the dictionary that maps\n",
    "        each index name with the path of the directory containing the relative json files.\n",
    "    '''\n",
    "    ret = dict()\n",
    "    for idx_dir in os.listdir(path):\n",
    "        if idx_dir.startswith('.'): continue\n",
    "        idx_path = os.path.join(path,idx_dir)\n",
    "        ret[idx_dir] = idx_path\n",
    "    return ret\n",
    "actual_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def join_list_of_ch_vc(col_name, df):\n",
    "    return df.apply(lambda row: ' '.join(ast.literal_eval(row[col_name])), \n",
    "                                axis = 'columns')\n",
    "\n",
    "def join_list_of_staff(df):\n",
    "    return df.apply(lambda row: ' '.join([el[0] for el in ast.literal_eval(row['staff'])]), \n",
    "                                axis = 'columns')\n",
    "\n",
    "def preprocess_column(col_name, df):\n",
    "    return df.apply(lambda row: (preprocess(row[col_name], SnowballStemmer('english'))), axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_staff(df):\n",
    "    string_staff = df.apply(lambda row: ' '.join([el[0] for el in ast.literal_eval(row['staff'])]), \n",
    "                                axis = 'columns')\n",
    "    print(f\"[Staff]: Converted all the lists in strings, starting the preprocessing...\")\n",
    "    prepr_staff = df.apply(lambda row: (preprocess(row['staff'], SnowballStemmer('english'))), axis = 'columns')\n",
    "    return prepr_staff\n",
    "\n",
    "def preprocessing_voices(df):\n",
    "    string_voices = df.apply(lambda row: ' '.join(ast.literal_eval(row['voices'])), \n",
    "                                    axis = 'columns')\n",
    "    print(f\"[Voices]: Converted all the lists in strings, starting the preprocessing...\")\n",
    "    return df.apply(lambda row: (preprocess(row['voices'], SnowballStemmer('english'))), axis = 'columns')\n",
    "\n",
    "def preprocessing_characters(df):\n",
    "    string_characters = df.apply(lambda row: ' '.join(ast.literal_eval(row['characters'])), \n",
    "                                    axis = 'columns')\n",
    "    print(f\"[Characters]: Converted all the lists in strings, starting the preprocessing...\")\n",
    "    return df.apply(lambda row: (preprocess(row['characters'], SnowballStemmer('english'))), axis = 'columns')\n",
    "\n",
    "def preprocessing_title(df):\n",
    "    print(f\"[Title]: Starting the preprocessing...\")\n",
    "    return df.apply(lambda row: (preprocess(row['title'], SnowballStemmer('english'))), axis = 'columns')\n",
    "\n",
    "def preprocessing_synopsis(df):\n",
    "    print(f\"[Synopsis]: Starting the preprocessing...\")\n",
    "    return df.apply(lambda row: (preprocess(row['synopsis'], SnowballStemmer('english'))), axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Staff]: Converted all the lists in strings, starting the preprocessing...\n",
      "[Staff]: Done.\n",
      "[Voices]: Converted all the lists in strings, starting the preprocessing...\n",
      "[Voices]: Done.\n",
      "[Characters]: Converted all the lists in strings, starting the preprocessing...\n",
      "[Characters]: Done.\n",
      "[Title]: Starting the preprocessing...\n",
      "[Title]: Done.\n"
     ]
    }
   ],
   "source": [
    "prepr_staff = preprocessing_staff(df)\n",
    "print(\"[Staff]: Done.\")\n",
    "\n",
    "prepr_voices = preprocessing_voices(df)\n",
    "print(\"[Voices]: Done.\")\n",
    "\n",
    "prepr_characters = preprocessing_characters(df)\n",
    "print(\"[Characters]: Done.\")\n",
    "\n",
    "prepr_title = preprocessing_title(df)\n",
    "print(\"[Title]: Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Staff]: Created vocabulary\n",
      "[Voices]: Created vocabulary\n",
      "[Characters]: Created vocabulary\n",
      "[Title]: Created vocabulary\n"
     ]
    }
   ],
   "source": [
    "voc_staff = create_vocab(prepr_staff)\n",
    "print(\"[Staff]: Created vocabulary\")\n",
    "\n",
    "voc_voices = create_vocab(prepr_voices)\n",
    "print(\"[Voices]: Created vocabulary\")\n",
    "\n",
    "voc_characters = create_vocab(prepr_characters)\n",
    "print(\"[Characters]: Created vocabulary\")\n",
    "\n",
    "voc_title = create_vocab(prepr_title)\n",
    "print(\"[Title]: Created vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Staff]: Created index\n",
      "[Voices]: Created index\n",
      "[Characters]: Created index\n",
      "[Title]: Created index\n"
     ]
    }
   ],
   "source": [
    "idx_staff = create_inv_idx(prepr_staff, voc_staff)\n",
    "print(\"[Staff]: Created index\")\n",
    "\n",
    "idx_voices = create_inv_idx(prepr_voices, voc_voices)\n",
    "print(\"[Voices]: Created index\")\n",
    "\n",
    "idx_characters = create_inv_idx(prepr_characters, voc_characters)\n",
    "print(\"[Characters]: Created index\")\n",
    "\n",
    "idx_title = create_inv_idx(prepr_title, voc_title)\n",
    "print(\"[Title]: Created index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dir = os.path.join('..', 'shared_stuff', 'indexes')\n",
    "staff_dir = os.path.join(idx_dir, 'staff')\n",
    "voices_dir = os.path.join(idx_dir, 'voices')\n",
    "ch_dir = os.path.join(idx_dir, 'characters')\n",
    "title_dir = os.path.join(idx_dir, 'title')\n",
    "syns_dir = os.path.join(idx_dir, 'synopsis')\n",
    "\n",
    "tot_dirs = {'staff': staff_dir, 'voices': voices_dir, 'characters': ch_dir, 'title': title_dir}\n",
    "for d in tot_dirs.values():\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Staff]: All saved.\n",
      "[Voices]: All saved.\n",
      "[Characters]: All saved.\n",
      "[Title]: All saved.\n"
     ]
    }
   ],
   "source": [
    "save_dict_to_file(dct=voc_staff, filename=os.path.join(staff_dir, 'vocabulary.json'))\n",
    "save_dict_to_file(dct=idx_staff, filename=os.path.join(staff_dir, 'inv_idx.json'))\n",
    "print(\"[Staff]: All saved.\")\n",
    "\n",
    "save_dict_to_file(dct=voc_voices, filename=os.path.join(voices_dir, 'vocabulary.json'))\n",
    "save_dict_to_file(dct=idx_voices, filename=os.path.join(voices_dir, 'inv_idx.json'))\n",
    "print(\"[Voices]: All saved.\")\n",
    "\n",
    "save_dict_to_file(dct=voc_characters, filename=os.path.join(ch_dir, 'vocabulary.json'))\n",
    "save_dict_to_file(dct=idx_characters, filename=os.path.join(ch_dir, 'inv_idx.json'))\n",
    "print(\"[Characters]: All saved.\")\n",
    "\n",
    "save_dict_to_file(dct=voc_title, filename=os.path.join(title_dir, 'vocabulary.json'))\n",
    "save_dict_to_file(dct=idx_title, filename=os.path.join(title_dir, 'inv_idx.json'))\n",
    "print(\"[Title]: All saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synopsis]: Starting the preprocessing...\n",
      "[Synopsis]: Done.\n",
      "[Synopsis]: Created vocabulary\n",
      "[Synopsis]: Created index\n"
     ]
    }
   ],
   "source": [
    "prepr_syns = preprocessing_synopsis(df)\n",
    "print(\"[Synopsis]: Done.\")\n",
    "voc_syns = create_vocab(prepr_syns)\n",
    "print(\"[Synopsis]: Created vocabulary\")\n",
    "idx_syns = create_inv_idx(prepr_syns, voc_syns)\n",
    "print(\"[Synopsis]: Created index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synopsis]: All saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "save_dict_to_file(dct=voc_syns, filename=os.path.join(syns_dir, 'vocabulary.json'))\n",
    "save_dict_to_file(dct=idx_syns, filename=os.path.join(syns_dir, 'inv_idx.json'))\n",
    "print(\"[Synopsis]: All saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atarou': 0,\n",
       " 'kisaku': 1,\n",
       " 'amigo': 2,\n",
       " 'bigotr': 3,\n",
       " 'pant': 4,\n",
       " 'hitotsubu': 5,\n",
       " 'camp': 6,\n",
       " 'builder': 7,\n",
       " 'tame': 8,\n",
       " 'bachi': 9,\n",
       " 'moburi-san': 10,\n",
       " 'oretacha': 11,\n",
       " 'plant': 12,\n",
       " 'greenpea': 13,\n",
       " 'waresho': 14,\n",
       " 'saishuushou': 15,\n",
       " 'tile': 16,\n",
       " 'pompo-san': 17,\n",
       " 'uwasa': 18,\n",
       " 'agateram': 19,\n",
       " 'templ': 20,\n",
       " 'kyoshin': 21,\n",
       " 'lianmeng': 22,\n",
       " 'chaguchagu': 23,\n",
       " 'coffi': 24,\n",
       " 'hasshin': 25,\n",
       " 'fuuun': 26,\n",
       " 'kaeta': 27,\n",
       " 'kumamoto': 28,\n",
       " 'fangshenshi': 29,\n",
       " 'gru': 30,\n",
       " 'et': 31,\n",
       " 'antholog': 32,\n",
       " 'amar': 33,\n",
       " 'hoshino-kun': 34,\n",
       " 'kabutt': 35,\n",
       " 'guupa': 36,\n",
       " 'tatakai': 37,\n",
       " 'tiantian': 38,\n",
       " 'naoko': 39,\n",
       " '.hack//g.u': 40,\n",
       " 'buchichi': 41,\n",
       " 'yoru': 42,\n",
       " 'hayat': 43,\n",
       " 'powerpuff': 44,\n",
       " 'enshuu': 45,\n",
       " 'mikosuri': 46,\n",
       " 'realiz': 47,\n",
       " 'chibo': 48,\n",
       " 'samas': 49,\n",
       " 'fuyumichi': 50,\n",
       " 'peacekeep': 51,\n",
       " 'atsumar': 52,\n",
       " 'kfc': 53,\n",
       " 't': 54,\n",
       " 'sizq': 55,\n",
       " 'naoto-hen': 56,\n",
       " 'youtai': 57,\n",
       " 'pinoko': 58,\n",
       " 'ressha-hen': 59,\n",
       " 'raifuku': 60,\n",
       " 'veri': 61,\n",
       " 'click': 62,\n",
       " 'jouhou': 63,\n",
       " 'child': 64,\n",
       " 'xiangyao': 65,\n",
       " 'nekko-kun': 66,\n",
       " 'mushibugyo': 67,\n",
       " 'dahua': 68,\n",
       " 'kawaijuku': 69,\n",
       " 'moco': 70,\n",
       " 'd.c.iii': 71,\n",
       " 'structur': 72,\n",
       " 'jim': 73,\n",
       " 'shashinkan': 74,\n",
       " 'shitsurakuen': 75,\n",
       " 'reiteki': 76,\n",
       " 'sailor-fuku': 77,\n",
       " 'orenchi': 78,\n",
       " 'famill': 79,\n",
       " 'like': 80,\n",
       " 'yashahim': 81,\n",
       " 'fuyuyasumi': 82,\n",
       " 'amakakeru': 83,\n",
       " 'utaou': 84,\n",
       " 'high☆spe': 85,\n",
       " 'rabbitub': 86,\n",
       " 'zeninshuugou': 87,\n",
       " 'kitakaz': 88,\n",
       " 'yobikou': 89,\n",
       " 'kuroki': 90,\n",
       " 'fatz': 91,\n",
       " 'shinki': 92,\n",
       " 'spirit': 93,\n",
       " 'seraph': 94,\n",
       " 'aki': 95,\n",
       " 'judo': 96,\n",
       " 'hikaruon': 97,\n",
       " 'tanuki-bayashi': 98,\n",
       " 'wars/forc': 99,\n",
       " 'groov': 100,\n",
       " 'kusog': 101,\n",
       " 'tamageta-kun': 102,\n",
       " 'zhaoqin': 103,\n",
       " 'sola': 104,\n",
       " 'mito': 105,\n",
       " 'ell': 106,\n",
       " 'execut': 107,\n",
       " 'dokuro': 108,\n",
       " 'roommat': 109,\n",
       " 'accomplic': 110,\n",
       " 'tsuzukunja': 111,\n",
       " 'kantarou': 112,\n",
       " 'isogashi': 113,\n",
       " 'shita-kiri': 114,\n",
       " 'cicada': 115,\n",
       " 'escalay': 116,\n",
       " 'bijukubo': 117,\n",
       " 'hareluya': 118,\n",
       " 'clumsi': 119,\n",
       " 'remix': 120,\n",
       " 'sabaku': 121,\n",
       " 'draw': 122,\n",
       " 'furikaeru': 123,\n",
       " 'toozakaru': 124,\n",
       " 'juju': 125,\n",
       " 'azusa': 126,\n",
       " 'crono': 127,\n",
       " 'houjou': 128,\n",
       " 'doku': 129,\n",
       " 'yoko': 130,\n",
       " 'gaik': 131,\n",
       " 'picnic': 132,\n",
       " 'chijoku': 133,\n",
       " 'shougekijou': 134,\n",
       " 'kihon': 135,\n",
       " 'tsuma': 136,\n",
       " 'nyaruko': 137,\n",
       " 'aisuru': 138,\n",
       " 'dolphin': 139,\n",
       " 'otamajakushi': 140,\n",
       " 'furifur': 141,\n",
       " 'biburi': 142,\n",
       " 'uzamaid': 143,\n",
       " 'zai': 144,\n",
       " 'sourou': 145,\n",
       " 'zhengyi': 146,\n",
       " 'koeta': 147,\n",
       " 'kudasai': 148,\n",
       " 'equat': 149,\n",
       " 'hoero': 150,\n",
       " 'nasuno': 151,\n",
       " 'choukoukaku': 152,\n",
       " 'tanita': 153,\n",
       " 'suizokukan': 154,\n",
       " 'genzou': 155,\n",
       " 'daisuugeki': 156,\n",
       " 'shadowvers': 157,\n",
       " 'kotengu': 158,\n",
       " '♭': 159,\n",
       " 'contact': 160,\n",
       " 'judi': 161,\n",
       " 'luan': 162,\n",
       " 'calm': 163,\n",
       " 'chakapoko': 164,\n",
       " 'burglar': 165,\n",
       " 'souhaku': 166,\n",
       " 'zenkoku-hen': 167,\n",
       " 'sunabouzu': 168,\n",
       " 'fuyu→kotatsu←haru': 169,\n",
       " 'swimmi': 170,\n",
       " 'slap': 171,\n",
       " 'soudansho': 172,\n",
       " 'countdown': 173,\n",
       " 'kougunka': 174,\n",
       " 'akashi': 175,\n",
       " 'kyokujitsu': 176,\n",
       " 'bakeri': 177,\n",
       " 'kuufuku': 178,\n",
       " 'joutoku': 179,\n",
       " 'tensura': 180,\n",
       " 'tangguo': 181,\n",
       " 'pokk': 182,\n",
       " 'lull': 183,\n",
       " 'genma': 184,\n",
       " 'teikoku': 185,\n",
       " 'd.gray-man': 186,\n",
       " 'straw-saurus': 187,\n",
       " 'genkina': 188,\n",
       " 'bus': 189,\n",
       " 'ladybird': 190,\n",
       " 'rocco': 191,\n",
       " 'sis': 192,\n",
       " 'tobaso': 193,\n",
       " 'soresaemo': 194,\n",
       " 'jisho': 195,\n",
       " 'yogosu': 196,\n",
       " 'nurs': 197,\n",
       " 'harenchi': 198,\n",
       " 'sorairo': 199,\n",
       " 'okashi': 200,\n",
       " 'dattebayo': 201,\n",
       " 'sume': 202,\n",
       " 'kioka': 203,\n",
       " 'run=dim': 204,\n",
       " 'kingdom': 205,\n",
       " '~after': 206,\n",
       " 'notat': 207,\n",
       " \"'m\": 208,\n",
       " 'zeno': 209,\n",
       " 'shiosai': 210,\n",
       " 'shirokuma': 211,\n",
       " 'druaga': 212,\n",
       " 'batter': 213,\n",
       " 'yadomeshi': 214,\n",
       " 'riversid': 215,\n",
       " 'rowdi': 216,\n",
       " 'joban': 217,\n",
       " 'dainanaka': 218,\n",
       " 'dokyou': 219,\n",
       " 'spyce': 220,\n",
       " 'ulibyeol': 221,\n",
       " 'overbloom': 222,\n",
       " 'hachimitsu': 223,\n",
       " 'yuuhi': 224,\n",
       " 'kenta': 225,\n",
       " 'yonimo': 226,\n",
       " 'teatrino': 227,\n",
       " 'pino': 228,\n",
       " 'ookamida': 229,\n",
       " 'liner': 230,\n",
       " 'lucario': 231,\n",
       " 'maniac': 232,\n",
       " 'gurentai': 233,\n",
       " 'bakabon': 234,\n",
       " 'kubozono': 235,\n",
       " 'gentei': 236,\n",
       " 'mell': 237,\n",
       " 'vanguard': 238,\n",
       " 'takkyuubu': 239,\n",
       " 'diqiu': 240,\n",
       " 'suisougaku-bu': 241,\n",
       " 'ongakukai': 242,\n",
       " 'mage': 243,\n",
       " 'file-x': 244,\n",
       " 'tourism': 245,\n",
       " 'huli': 246,\n",
       " 'tottemo': 247,\n",
       " 'uru': 248,\n",
       " 'luvilia': 249,\n",
       " 'yome-san': 250,\n",
       " 'gunsou': 251,\n",
       " 'kuzu': 252,\n",
       " 'tachiagar': 253,\n",
       " 'amamikoshima': 254,\n",
       " 'dongtian': 255,\n",
       " 'eromam': 256,\n",
       " 'yahari': 257,\n",
       " 'nanahiki': 258,\n",
       " 'shuuchakueki': 259,\n",
       " 'grace': 260,\n",
       " 'misemasu': 261,\n",
       " 'excellion': 262,\n",
       " 'yagi': 263,\n",
       " 'kinkreet': 264,\n",
       " 'zuo': 265,\n",
       " 'ufo': 266,\n",
       " 'nanaana': 267,\n",
       " 'royalti': 268,\n",
       " 'inuyama': 269,\n",
       " 'imperi': 270,\n",
       " 'diao': 271,\n",
       " 'daihanran': 272,\n",
       " 'wengzhong': 273,\n",
       " 'millennia': 274,\n",
       " 'warui': 275,\n",
       " 'kreuz': 276,\n",
       " 'nisha': 277,\n",
       " 'ninjutsu': 278,\n",
       " 'ichizoku': 279,\n",
       " 'short': 280,\n",
       " 'huan': 281,\n",
       " 'monogram': 282,\n",
       " 'sushi': 283,\n",
       " 'tactic': 284,\n",
       " 'madoromi': 285,\n",
       " 'reboot': 286,\n",
       " 'realist': 287,\n",
       " 'baikinman': 288,\n",
       " 'kiit': 289,\n",
       " 'senlin': 290,\n",
       " 'kurenai': 291,\n",
       " 'mechadock': 292,\n",
       " 'monologu': 293,\n",
       " 'yuuna-san': 294,\n",
       " 'sebangou': 295,\n",
       " 'bunna': 296,\n",
       " 'minwa': 297,\n",
       " 'stud': 298,\n",
       " 'entaku': 299,\n",
       " 'sofyrubi': 300,\n",
       " 'kuro-nikuru': 301,\n",
       " 'oraretara': 302,\n",
       " 'fencer': 303,\n",
       " 'kita': 304,\n",
       " 'arcadia-gou': 305,\n",
       " 'drugstor': 306,\n",
       " 'choubatsu': 307,\n",
       " 'er': 308,\n",
       " 'kaseifu': 309,\n",
       " 'monsterz': 310,\n",
       " 'kaikitan': 311,\n",
       " 'funbari': 312,\n",
       " 'portriss': 313,\n",
       " 'town': 314,\n",
       " 'taisenki': 315,\n",
       " 'kanan': 316,\n",
       " 'meifon': 317,\n",
       " 'qicai': 318,\n",
       " 'dalbitkungkwol': 319,\n",
       " 'haedori': 320,\n",
       " 'kegasareru': 321,\n",
       " 'geum': 322,\n",
       " 'max': 323,\n",
       " 'brief': 324,\n",
       " 'senkitan': 325,\n",
       " 'eiken': 326,\n",
       " 'yunh': 327,\n",
       " 'uranof': 328,\n",
       " 'popolocroi': 329,\n",
       " 'benkai': 330,\n",
       " 'choral': 331,\n",
       " 'nakanaori': 332,\n",
       " 'shiteiseki': 333,\n",
       " 'urasekai': 334,\n",
       " 'masaiban': 335,\n",
       " 'tamag': 336,\n",
       " 'hachidori': 337,\n",
       " 'kizuato': 338,\n",
       " 'dept': 339,\n",
       " 'shuzi': 340,\n",
       " 'someon': 341,\n",
       " 'omishi': 342,\n",
       " 'umareta': 343,\n",
       " 'madoka': 344,\n",
       " 'theme': 345,\n",
       " 'shiton': 346,\n",
       " 'asaki': 347,\n",
       " 'pandra': 348,\n",
       " 'claus': 349,\n",
       " 'cruso': 350,\n",
       " 'abe': 351,\n",
       " 'dagram': 352,\n",
       " 'norabbit': 353,\n",
       " 'chicchai': 354,\n",
       " 'itsuka': 355,\n",
       " 'hell': 356,\n",
       " 'tunshi': 357,\n",
       " 'ryuuken': 358,\n",
       " 'yuunagi': 359,\n",
       " 'pelican': 360,\n",
       " 'haritsuk': 361,\n",
       " 'bananajima': 362,\n",
       " 'tsudou': 363,\n",
       " 'momokyun': 364,\n",
       " 'rosario': 365,\n",
       " \"h'or\": 366,\n",
       " 'ryuukoku': 367,\n",
       " 'kokomom': 368,\n",
       " 'smart': 369,\n",
       " 'laid': 370,\n",
       " 'sousamou': 371,\n",
       " 'kokoro-chan': 372,\n",
       " 'yuyushiki': 373,\n",
       " '.koni-chan': 374,\n",
       " 'kagakusha-tachi': 375,\n",
       " 'mihoshi': 376,\n",
       " 'omoshiroi': 377,\n",
       " 'million': 378,\n",
       " 'kageokuri': 379,\n",
       " 'rahmen': 380,\n",
       " 'epic': 381,\n",
       " 'spiral': 382,\n",
       " 'entei': 383,\n",
       " 'fudanshi': 384,\n",
       " 'aah': 385,\n",
       " 'ittokiya': 386,\n",
       " 'erolut': 387,\n",
       " 'juli': 388,\n",
       " 'spriggan': 389,\n",
       " 'assiz': 390,\n",
       " 'arara': 391,\n",
       " 'ppalgan': 392,\n",
       " 'gakusav': 393,\n",
       " 'casshern': 394,\n",
       " 'nihonichi': 395,\n",
       " 'oshushidayo': 396,\n",
       " 'k×drop': 397,\n",
       " 'ometeotl': 398,\n",
       " 'beani': 399,\n",
       " 'doubutsu': 400,\n",
       " 'anoth': 401,\n",
       " 'sama': 402,\n",
       " 'kangoku': 403,\n",
       " 'aldnoah.zero': 404,\n",
       " 'ouma': 405,\n",
       " 'rhea': 406,\n",
       " 'elvi': 407,\n",
       " 'axi': 408,\n",
       " 'shougakusei': 409,\n",
       " 'gaist': 410,\n",
       " 'cubix': 411,\n",
       " 'tcg': 412,\n",
       " 'fuiteru': 413,\n",
       " 'the☆doraemon': 414,\n",
       " 'cazador': 415,\n",
       " 'dougo': 416,\n",
       " 'kashiwa': 417,\n",
       " 'petro': 418,\n",
       " 'fushigi': 419,\n",
       " 'moeru': 420,\n",
       " 'yan': 421,\n",
       " 'paripari': 422,\n",
       " 'kiniitta': 423,\n",
       " 'u.c': 424,\n",
       " 'korekarasaki': 425,\n",
       " 'koet': 426,\n",
       " 'eikoden': 427,\n",
       " 'bookmark': 428,\n",
       " 'cesar': 429,\n",
       " 'eightraid': 430,\n",
       " 'waratt': 431,\n",
       " 'rainbowman': 432,\n",
       " 'patron': 433,\n",
       " 'eiga-sai': 434,\n",
       " 'zuma': 435,\n",
       " 'tatta': 436,\n",
       " 'fujiyama': 437,\n",
       " 'nobushi': 438,\n",
       " 'tamago': 439,\n",
       " 'satisfact': 440,\n",
       " 'jiisan': 441,\n",
       " 'inspector': 442,\n",
       " 'nyuu': 443,\n",
       " 'jimiko': 444,\n",
       " 'keito': 445,\n",
       " 'bewar': 446,\n",
       " 'tanteidokoro': 447,\n",
       " 'awak': 448,\n",
       " 'asatarou': 449,\n",
       " 'mura': 450,\n",
       " 'nukiuchi': 451,\n",
       " 'jeog-en': 452,\n",
       " 'lagoon': 453,\n",
       " 'chikku': 454,\n",
       " 'biyaku': 455,\n",
       " 'afghanistan': 456,\n",
       " 'revevolut': 457,\n",
       " 'tie': 458,\n",
       " 'majutsu': 459,\n",
       " 'oyayubi-him': 460,\n",
       " 'mignon': 461,\n",
       " 'mamma★mia': 462,\n",
       " 'carlvinson': 463,\n",
       " 'talent': 464,\n",
       " 'nanoru': 465,\n",
       " 'guilai': 466,\n",
       " 'chawan': 467,\n",
       " 'dairinin': 468,\n",
       " 'chiyoko': 469,\n",
       " 'shenzhou': 470,\n",
       " 'korakuen': 471,\n",
       " 'coaster': 472,\n",
       " 'tsum': 473,\n",
       " 'elmer': 474,\n",
       " 'matsuei': 475,\n",
       " 'kotaro': 476,\n",
       " 'kabushiki': 477,\n",
       " 'yeohaengja': 478,\n",
       " 'taoqi': 479,\n",
       " 'gochou': 480,\n",
       " 'owakare-kai': 481,\n",
       " 'wanghu': 482,\n",
       " 'tomoyo-hen': 483,\n",
       " 'konohamaru': 484,\n",
       " 'tsuki-sama': 485,\n",
       " 'oheya': 486,\n",
       " 'break-ag': 487,\n",
       " 'chiguhagu': 488,\n",
       " 'tenjhoteng': 489,\n",
       " 'shikoyaka': 490,\n",
       " 'melti': 491,\n",
       " 'baraou': 492,\n",
       " 'plawr': 493,\n",
       " 'heidi': 494,\n",
       " 'otoboku': 495,\n",
       " 'jutai': 496,\n",
       " 'ooi': 497,\n",
       " 'natsukashii': 498,\n",
       " 'ryouriten': 499,\n",
       " 'tobasu': 500,\n",
       " 'todoufuken': 501,\n",
       " 'kedo': 502,\n",
       " 'echigo': 503,\n",
       " 'ball': 504,\n",
       " 'kusuo': 505,\n",
       " 'yuki-chan': 506,\n",
       " 'shabak': 507,\n",
       " 'ojaru': 508,\n",
       " 'debt': 509,\n",
       " 'yin': 510,\n",
       " 'mezashita': 511,\n",
       " 'bo': 512,\n",
       " 'otoridai': 513,\n",
       " 'baenang-yeohaeng': 514,\n",
       " 'strawberri': 515,\n",
       " 'street': 516,\n",
       " 'zhu': 517,\n",
       " 'mourn': 518,\n",
       " 'juushi': 519,\n",
       " 'lulu': 520,\n",
       " 'urmă': 521,\n",
       " 'kayou': 522,\n",
       " 'hanazono': 523,\n",
       " 'detroit': 524,\n",
       " 'tuzki': 525,\n",
       " 'kyutai': 526,\n",
       " 'yewai': 527,\n",
       " 'kikuko': 528,\n",
       " 'kitsutsuki': 529,\n",
       " 'byoutou': 530,\n",
       " 'oosouji': 531,\n",
       " 'medley': 532,\n",
       " 'edg': 533,\n",
       " 'pierrot': 534,\n",
       " 'poroporo': 535,\n",
       " 'mushakero': 536,\n",
       " 'ontama': 537,\n",
       " 'chizuru-chan': 538,\n",
       " 'cord': 539,\n",
       " 'jameseo': 540,\n",
       " 'jobuubu': 541,\n",
       " 'shidoushit': 542,\n",
       " 'jaku-chara': 543,\n",
       " 'daojian': 544,\n",
       " 'kowaku': 545,\n",
       " 'crew': 546,\n",
       " 'stella': 547,\n",
       " 'tenta': 548,\n",
       " 'let': 549,\n",
       " 'ganbaruzu': 550,\n",
       " 'kindergarten': 551,\n",
       " 'kosi': 552,\n",
       " 'seieki♥': 553,\n",
       " 'taerang': 554,\n",
       " 'maetel': 555,\n",
       " 'bakegyamon': 556,\n",
       " 'shoukoujo': 557,\n",
       " 'ayane-chan': 558,\n",
       " 'mortal': 559,\n",
       " 'kun': 560,\n",
       " 'ijim': 561,\n",
       " 'nanbo': 562,\n",
       " 'hanbun': 563,\n",
       " 'kuukijuu': 564,\n",
       " 'junk': 565,\n",
       " 'ggo': 566,\n",
       " 'mina': 567,\n",
       " 'daniel': 568,\n",
       " 'sam': 569,\n",
       " 'nendjuugyouji': 570,\n",
       " 'extra-terrestri': 571,\n",
       " 'douryoku': 572,\n",
       " 'atta': 573,\n",
       " 'newmanoid': 574,\n",
       " 'nostradamus': 575,\n",
       " 'suzakinishi': 576,\n",
       " 'mohuan': 577,\n",
       " 'tadaiku': 578,\n",
       " 'maman': 579,\n",
       " 'jiru': 580,\n",
       " 'arthur': 581,\n",
       " 'inkou': 582,\n",
       " 'reconcili': 583,\n",
       " 'tsunami': 584,\n",
       " 'hoshiai': 585,\n",
       " 'greed': 586,\n",
       " 'nareru': 587,\n",
       " 'cuticl': 588,\n",
       " 'warenai': 589,\n",
       " 'ashiaraiyashiki': 590,\n",
       " 'yumearu': 591,\n",
       " 'calpi': 592,\n",
       " 'lite': 593,\n",
       " 'hiraku': 594,\n",
       " 'housnail': 595,\n",
       " 'keshiko': 596,\n",
       " 'itadaki': 597,\n",
       " 'koeru': 598,\n",
       " 'dokkaebi': 599,\n",
       " 'gloria': 600,\n",
       " 'sasami': 601,\n",
       " 'gakushou': 602,\n",
       " 'annai': 603,\n",
       " 'nibelung': 604,\n",
       " 'mianhada': 605,\n",
       " 'rifujin-jin': 606,\n",
       " 'superband': 607,\n",
       " 'keikaku': 608,\n",
       " 'colocolo': 609,\n",
       " 'naruto': 610,\n",
       " 'kaleido': 611,\n",
       " 'hakken': 612,\n",
       " 'luna': 613,\n",
       " 'binchou-tan': 614,\n",
       " 'nusumareta': 615,\n",
       " 'tokei': 616,\n",
       " 'dededen': 617,\n",
       " 'rurouni': 618,\n",
       " 'plane': 619,\n",
       " 'brother': 620,\n",
       " 'touhoku-hen': 621,\n",
       " 'ryohei': 622,\n",
       " 'blossom~': 623,\n",
       " 'sad': 624,\n",
       " 'minai': 625,\n",
       " 'gilgamesh': 626,\n",
       " 'tomoko': 627,\n",
       " 'pappara': 628,\n",
       " 'ikikata': 629,\n",
       " 'konsui': 630,\n",
       " 'sakura': 631,\n",
       " 's.t.a.r.t': 632,\n",
       " 'kyakkya': 633,\n",
       " 'senko-san': 634,\n",
       " 'nymphomaniac': 635,\n",
       " 'ralli': 636,\n",
       " 'kakushi': 637,\n",
       " 'itou': 638,\n",
       " 'chitose-hen': 639,\n",
       " 'unison': 640,\n",
       " 'suru': 641,\n",
       " 'shigutsman': 642,\n",
       " 'hanaukyou': 643,\n",
       " 'fumai-kou': 644,\n",
       " 'chuutou-bu': 645,\n",
       " 'yotoden': 646,\n",
       " 'okashina': 647,\n",
       " 'imari': 648,\n",
       " 'dommel': 649,\n",
       " 'hako': 650,\n",
       " 'robo-rumbl': 651,\n",
       " 'genocyb': 652,\n",
       " 'xuzhang': 653,\n",
       " 'iwatobi': 654,\n",
       " 'shake': 655,\n",
       " 'suttoko': 656,\n",
       " 'unsu': 657,\n",
       " 'danganronpa': 658,\n",
       " 'chai': 659,\n",
       " 'futakomori': 660,\n",
       " 'salamand': 661,\n",
       " 'shuto': 662,\n",
       " 'hiros': 663,\n",
       " 'moero': 664,\n",
       " 'dogiragon': 665,\n",
       " 'beullaeg-eosseolteuui': 666,\n",
       " 'teardrop': 667,\n",
       " 'chengxia': 668,\n",
       " 'darken': 669,\n",
       " 'nuigurumi': 670,\n",
       " 'york': 671,\n",
       " 'shining☆rom': 672,\n",
       " 'gali': 673,\n",
       " 'privat': 674,\n",
       " 'y': 675,\n",
       " 'lue': 676,\n",
       " 'bakukyuu': 677,\n",
       " 'hikiko': 678,\n",
       " 'elus': 679,\n",
       " 'shinemon-hen': 680,\n",
       " 'zhipai': 681,\n",
       " 'g-best': 682,\n",
       " 'mametarou': 683,\n",
       " 'kobuta': 684,\n",
       " 'konogoro': 685,\n",
       " 'fog': 686,\n",
       " 'ippun': 687,\n",
       " 'corn': 688,\n",
       " 'gyo': 689,\n",
       " 'robokko': 690,\n",
       " 'chokkyuu': 691,\n",
       " 'yuujinchou': 692,\n",
       " 'googuri': 693,\n",
       " 'saiyajin': 694,\n",
       " 'viewti': 695,\n",
       " 'b.e.e': 696,\n",
       " 'mizugi': 697,\n",
       " 'kininatt': 698,\n",
       " 'ma-eum-uisoli': 699,\n",
       " 'tsuukin': 700,\n",
       " 'kkaeeoboni': 701,\n",
       " 'katasumi': 702,\n",
       " 'onnanoko': 703,\n",
       " 'ijirar': 704,\n",
       " 'chiharu': 705,\n",
       " 'deji': 706,\n",
       " 'arpo': 707,\n",
       " 'sunghyungsoo': 708,\n",
       " 'poka': 709,\n",
       " 'baldr': 710,\n",
       " 'oishi-sa': 711,\n",
       " 'takarabako': 712,\n",
       " 'chunfeng': 713,\n",
       " 'reimeiki': 714,\n",
       " 'meisei': 715,\n",
       " 'away': 716,\n",
       " 'sloven': 717,\n",
       " 'disney': 718,\n",
       " 'miyori': 719,\n",
       " \"girl's-fram\": 720,\n",
       " 'joybo': 721,\n",
       " 'mijikamon': 722,\n",
       " 'milk-chan': 723,\n",
       " 'koushi': 724,\n",
       " 'tennin': 725,\n",
       " 'teugsu': 726,\n",
       " 'kowashichatta': 727,\n",
       " 'tsuigeki': 728,\n",
       " 'puchim': 729,\n",
       " 'sansan': 730,\n",
       " 'sc': 731,\n",
       " 'champloo': 732,\n",
       " 'yinhun': 733,\n",
       " 'fumutto': 734,\n",
       " 'gordian': 735,\n",
       " 'geba': 736,\n",
       " 'paprika': 737,\n",
       " 'tankoufu': 738,\n",
       " '~su~mi~re~': 739,\n",
       " 'qingk': 740,\n",
       " 'linebarrel': 741,\n",
       " 'kattobi': 742,\n",
       " 'mazica': 743,\n",
       " 'jianianhua': 744,\n",
       " 'ingan': 745,\n",
       " 'shimauma': 746,\n",
       " 'aosa': 747,\n",
       " 'tokio': 748,\n",
       " 'macademi': 749,\n",
       " 'sonata': 750,\n",
       " 'dangorou': 751,\n",
       " 'wedding★combat': 752,\n",
       " 'shoujo☆kageki': 753,\n",
       " 'irokoi': 754,\n",
       " 'jigueseo': 755,\n",
       " 'angeltia': 756,\n",
       " 'discipl': 757,\n",
       " 'owari': 758,\n",
       " 'art': 759,\n",
       " 'kagerouka-kun': 760,\n",
       " 'hi.me.go.to': 761,\n",
       " 'doe': 762,\n",
       " 'gatenkei': 763,\n",
       " 'ke': 764,\n",
       " 'togainu': 765,\n",
       " 'ashi': 766,\n",
       " 'u-jin': 767,\n",
       " 'tong': 768,\n",
       " 'nashi': 769,\n",
       " 'naxi': 770,\n",
       " 'rilu': 771,\n",
       " 'gad': 772,\n",
       " 'chougekisen': 773,\n",
       " 'gendaijuu': 774,\n",
       " 'robocchi': 775,\n",
       " 'oraa': 776,\n",
       " 'bougyakusareta': 777,\n",
       " 'loui': 778,\n",
       " 'maxvalu': 779,\n",
       " 'o-o-o': 780,\n",
       " 'laura': 781,\n",
       " 'tumbl': 782,\n",
       " 'immort': 783,\n",
       " 'keiraku': 784,\n",
       " 'dwarf': 785,\n",
       " 'imakara': 786,\n",
       " 'sequenc': 787,\n",
       " 'muket': 788,\n",
       " 'mechapoli': 789,\n",
       " 'campanella': 790,\n",
       " 'makemag': 791,\n",
       " 'handsom': 792,\n",
       " 'makibao': 793,\n",
       " 'hanasakeru': 794,\n",
       " 'bone': 795,\n",
       " 'hijouji': 796,\n",
       " 'lia': 797,\n",
       " 'push': 798,\n",
       " 'phantomi': 799,\n",
       " 'virgin': 800,\n",
       " 'classmat': 801,\n",
       " 'kawarazaki-k': 802,\n",
       " 'nichijou-kei': 803,\n",
       " 'paizuri': 804,\n",
       " 'watashitolipton': 805,\n",
       " 'shinran': 806,\n",
       " 'yourself': 807,\n",
       " 'yomeiri': 808,\n",
       " 'yukiwatari': 809,\n",
       " 'harukaz': 810,\n",
       " 'dorami-chan': 811,\n",
       " 'mimi': 812,\n",
       " 'lover': 813,\n",
       " 'ekubo': 814,\n",
       " 'frontlin': 815,\n",
       " 'futarikiri': 816,\n",
       " 'laugh': 817,\n",
       " 'gargoyl': 818,\n",
       " 'higenashi': 819,\n",
       " 'bijutsubu': 820,\n",
       " 'mmo': 821,\n",
       " 'kota': 822,\n",
       " 'ergo': 823,\n",
       " 'ippai/boku': 824,\n",
       " 'badland': 825,\n",
       " 'sansha': 826,\n",
       " 'wolf': 827,\n",
       " 'key': 828,\n",
       " 'shiguang': 829,\n",
       " 'fragtim': 830,\n",
       " 'size': 831,\n",
       " 'bottom': 832,\n",
       " 'mcdull': 833,\n",
       " 'teleclub': 834,\n",
       " 'fuichin-san': 835,\n",
       " 'proclam': 836,\n",
       " 'nyanbo': 837,\n",
       " 'zuozhan': 838,\n",
       " 'kidzuki-hen': 839,\n",
       " 'dick': 840,\n",
       " 'uroko': 841,\n",
       " 'golfer': 842,\n",
       " 'tasuuketsu': 843,\n",
       " 'midori-ppoi': 844,\n",
       " 'betray': 845,\n",
       " 'kemono-tachi': 846,\n",
       " 'kakuchou': 847,\n",
       " 'kyouiku': 848,\n",
       " 'gai-hen': 849,\n",
       " 'juan': 850,\n",
       " 'kamigami': 851,\n",
       " 'lio': 852,\n",
       " 'baraki': 853,\n",
       " 'begin': 854,\n",
       " 'natsu-iro': 855,\n",
       " 'disguis': 856,\n",
       " 'kuruoshii': 857,\n",
       " 'proxi': 858,\n",
       " 'ushigaeru': 859,\n",
       " 're-main': 860,\n",
       " 'sugi': 861,\n",
       " 'darkstalk': 862,\n",
       " 'umezu': 863,\n",
       " 'iuna': 864,\n",
       " 'mutou': 865,\n",
       " 'kyoui': 866,\n",
       " 'shoshi': 867,\n",
       " 'mamonogatari': 868,\n",
       " 'mayb': 869,\n",
       " 'himitsu': 870,\n",
       " 'katsut': 871,\n",
       " 'shitennou': 872,\n",
       " 'kippu': 873,\n",
       " 'rika': 874,\n",
       " 'gts': 875,\n",
       " 'gensou-teki': 876,\n",
       " 'here': 877,\n",
       " 'tesapuru': 878,\n",
       " 'gokkun': 879,\n",
       " 'yanyi': 880,\n",
       " 'harri': 881,\n",
       " 'miteen': 882,\n",
       " 'dwaeji-ui': 883,\n",
       " 'bell': 884,\n",
       " 'iruka': 885,\n",
       " 'onshuu': 886,\n",
       " 'brynhildr': 887,\n",
       " 'ascens': 888,\n",
       " 'yanbou': 889,\n",
       " 'sweet': 890,\n",
       " 'sasuk': 891,\n",
       " 'crayon': 892,\n",
       " 'templex': 893,\n",
       " 'kimigayo': 894,\n",
       " 'moribito': 895,\n",
       " 'reideen': 896,\n",
       " 'kairiku': 897,\n",
       " 'narimasu': 898,\n",
       " 'cup': 899,\n",
       " 'doeeo': 900,\n",
       " 'gerent': 901,\n",
       " 'ichigo': 902,\n",
       " 'xiaojiang': 903,\n",
       " 'wata': 904,\n",
       " 'shiru': 905,\n",
       " 'an': 906,\n",
       " 'saranghanda': 907,\n",
       " 'bouquet': 908,\n",
       " 'toyo': 909,\n",
       " 'shinnyuusei': 910,\n",
       " 'manabi': 911,\n",
       " 'pud': 912,\n",
       " 'bakuen': 913,\n",
       " 'quartet': 914,\n",
       " 'loll': 915,\n",
       " 'xianluo': 916,\n",
       " 'be-boy': 917,\n",
       " 'scar': 918,\n",
       " 'kouryaku': 919,\n",
       " 'cultiv': 920,\n",
       " 'zange-roku': 921,\n",
       " 'nintama': 922,\n",
       " 'momiji': 923,\n",
       " 'begonia': 924,\n",
       " 'harimogu': 925,\n",
       " 'niplheim': 926,\n",
       " 'musubou': 927,\n",
       " 'youko': 928,\n",
       " 'nemuri': 929,\n",
       " 'asia': 930,\n",
       " 'kahey': 931,\n",
       " 'ichigeki': 932,\n",
       " 'shoukai': 933,\n",
       " 'yao': 934,\n",
       " 'möbius': 935,\n",
       " 'grid': 936,\n",
       " 'transfer': 937,\n",
       " 'sotsugyou': 938,\n",
       " 'ryuuseiki': 939,\n",
       " 'chen-kuro': 940,\n",
       " 'order': 941,\n",
       " 'handkerchief': 942,\n",
       " 'aeioudan': 943,\n",
       " 'kobo-chan': 944,\n",
       " 'isshitsu': 945,\n",
       " 'render': 946,\n",
       " 'magical☆shop': 947,\n",
       " 'agasa': 948,\n",
       " 'kitenakut': 949,\n",
       " 'reset': 950,\n",
       " 'utopa': 951,\n",
       " 'mother': 952,\n",
       " 'eren': 953,\n",
       " 'jimao': 954,\n",
       " 'schwarz': 955,\n",
       " 'ghast': 956,\n",
       " 'cogimyun': 957,\n",
       " 'alo': 958,\n",
       " 'voodooland': 959,\n",
       " 'iburi': 960,\n",
       " 'bunkiten': 961,\n",
       " 'rurou': 962,\n",
       " 'tonagura': 963,\n",
       " 'sewayaki': 964,\n",
       " 'domin': 965,\n",
       " 'virtuacal': 966,\n",
       " 'kyokashou': 967,\n",
       " 'miyano': 968,\n",
       " 'miracl': 969,\n",
       " 'samia': 970,\n",
       " 'leyuan': 971,\n",
       " 'rhythm/trump': 972,\n",
       " 'tsukeyou': 973,\n",
       " 'gundam': 974,\n",
       " 'viral': 975,\n",
       " 'tokorozawa': 976,\n",
       " 'willi': 977,\n",
       " 'time-patrol': 978,\n",
       " 'confront': 979,\n",
       " 'ant': 980,\n",
       " 'shirouto': 981,\n",
       " 'makaryuudo': 982,\n",
       " 'escaflown': 983,\n",
       " 'flow': 984,\n",
       " 'douga': 985,\n",
       " 'femal': 986,\n",
       " 'souiu': 987,\n",
       " 'juusenki': 988,\n",
       " 'kanchou': 989,\n",
       " 'lump': 990,\n",
       " 'masani': 991,\n",
       " 'kaitai': 992,\n",
       " 'shitou-hen': 993,\n",
       " 'purin': 994,\n",
       " 'aliv': 995,\n",
       " 'hiza': 996,\n",
       " 'fushisha': 997,\n",
       " 'choujigen': 998,\n",
       " 'spotlight': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "voc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def parse_advanced_query(query):\n",
    "    cum = []\n",
    "    ret = dict()\n",
    "    act_ind = actual_indexes()\n",
    "    print(query.split(' '))\n",
    "    for word in query.split(' '):\n",
    "        if word.startswith('['):\n",
    "            k = word[1:-1]\n",
    "            if k not in act_ind:\n",
    "                raise ValueError(f\"You cannot query the field {k}, you can only choose between: {list(act_ind.keys())}\")\n",
    "            ret[k] = cum\n",
    "            cum = []\n",
    "        else:\n",
    "            cum.append(word)\n",
    "    return {k: parse_query(v, read_dict_from_file(filename=os.path.join(act_ind[k], 'vocabulary.json'))) for k,v in ret.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dragon', '[title]', 'vegeta', '[characters]']\n",
      "{'title': [4998], 'characters': [14797]}\n"
     ]
    }
   ],
   "source": [
    "p_q = parse_advanced_query(\"dragon [title] vegeta [characters]\")\n",
    "print(p_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_results(query, inv_idx):\n",
    "def get_advanced_results(query_dict):\n",
    "    ret = dict()\n",
    "    act_ind = actual_indexes()\n",
    "    for k in query_dict:\n",
    "        inv_idx = read_dict_from_file(os.path.join(act_ind[k], 'inv_idx.json'))\n",
    "        if len(query_dict[k]) != 0:\n",
    "            ret[k] = get_results(query_dict[k], inv_idx)\n",
    "    \n",
    "    if len(ret)==0:\n",
    "        return set()\n",
    "    return set.intersection(*ret.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_len = {k: len(v) for k, v in parsed_query.items()}\n",
    "\n",
    "WEIGHT = {'title': 0.7, 'staff': 0.05, 'voices': 0.08, 'synopsis': 0.05, 'characters': 0.12}\n",
    "\n",
    "def score(df, query_len):\n",
    "    'title: len_queryper_title, staff: len_queryper_staff,  ...'\n",
    "    global WEIGHT\n",
    "    cum = 0\n",
    "    for i, q_i in query_len.items():\n",
    "        p_i = WEIGHT[i]   \n",
    "        d_i = len(df[i].split(' '))\n",
    "        cum += (q_i/d_i)*p_i\n",
    "    return cum\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_from_idx(idx, path='../shared_stuff/url_list.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        urls = f.readlines()\n",
    "    return urls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_printable_doc(df,idx, score):\n",
    "    ret = df.iloc[idx]\n",
    "    url = get_url_from_idx(idx)\n",
    "    ret['url'] = url\n",
    "    ret['score'] = score\n",
    "    return ret[['title', 'synopsis', 'url', 'score']].to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pippo', '[title]', 'dragon', '[voices]']\n",
      "The term 'pippo' wasn't found anywhere!\n",
      "The term 'dragon' wasn't found anywhere!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-23d7a1b19d6e>:9: UserWarning: Cannot find any anime for the query: 'pippo [title] dragon [voices]'\n",
      "  warnings.warn(f\"Cannot find any anime for the query: '{query}'\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_id, title, description, url, score]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def query_anime(df, query, k=None):\n",
    "    parsed_query = parse_advanced_query(query)\n",
    "    results = list(get_advanced_results(parsed_query))\n",
    "\n",
    "    if len(results)==0:\n",
    "        warnings.warn(f\"Cannot find any anime for the query: '{query}'\")\n",
    "        return pd.DataFrame(columns=['doc_id', 'title', 'description', 'url', 'score'])\n",
    "    query_len = {k: len(v) for k, v in parsed_query.items()}\n",
    "\n",
    "    scores = [(idx, score(df.iloc[idx], query_len)) for idx in results]\n",
    "\n",
    "    scores = sorted(scores, key=lambda coppia: -coppia[1])\n",
    "\n",
    "    # Selecting the first k elements\n",
    "    if k is None or k>len(scores):\n",
    "        k = len(scores)\n",
    "    scores = scores[:k]\n",
    "\n",
    "    return pd.concat(list(map(lambda tupla: get_printable_doc(df, *tupla), scores))).reset_index().rename(columns={\"index\": \"doc_id\", \"synopsis\": \"description\"})\n",
    "\n",
    "dragonball = \"pippo [title] dragon [voices]\"\n",
    "\n",
    "query_anime(df, dragonball)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atarou': 0,\n",
       " 'kisaku': 1,\n",
       " 'amigo': 2,\n",
       " 'bigotr': 3,\n",
       " 'pant': 4,\n",
       " 'hitotsubu': 5,\n",
       " 'camp': 6,\n",
       " 'builder': 7,\n",
       " 'tame': 8,\n",
       " 'bachi': 9,\n",
       " 'moburi-san': 10,\n",
       " 'oretacha': 11,\n",
       " 'plant': 12,\n",
       " 'greenpea': 13,\n",
       " 'waresho': 14,\n",
       " 'saishuushou': 15,\n",
       " 'tile': 16,\n",
       " 'pompo-san': 17,\n",
       " 'uwasa': 18,\n",
       " 'agateram': 19,\n",
       " 'templ': 20,\n",
       " 'kyoshin': 21,\n",
       " 'lianmeng': 22,\n",
       " 'chaguchagu': 23,\n",
       " 'coffi': 24,\n",
       " 'hasshin': 25,\n",
       " 'fuuun': 26,\n",
       " 'kaeta': 27,\n",
       " 'kumamoto': 28,\n",
       " 'fangshenshi': 29,\n",
       " 'gru': 30,\n",
       " 'et': 31,\n",
       " 'antholog': 32,\n",
       " 'amar': 33,\n",
       " 'hoshino-kun': 34,\n",
       " 'kabutt': 35,\n",
       " 'guupa': 36,\n",
       " 'tatakai': 37,\n",
       " 'tiantian': 38,\n",
       " 'naoko': 39,\n",
       " '.hack//g.u': 40,\n",
       " 'buchichi': 41,\n",
       " 'yoru': 42,\n",
       " 'hayat': 43,\n",
       " 'powerpuff': 44,\n",
       " 'enshuu': 45,\n",
       " 'mikosuri': 46,\n",
       " 'realiz': 47,\n",
       " 'chibo': 48,\n",
       " 'samas': 49,\n",
       " 'fuyumichi': 50,\n",
       " 'peacekeep': 51,\n",
       " 'atsumar': 52,\n",
       " 'kfc': 53,\n",
       " 't': 54,\n",
       " 'sizq': 55,\n",
       " 'naoto-hen': 56,\n",
       " 'youtai': 57,\n",
       " 'pinoko': 58,\n",
       " 'ressha-hen': 59,\n",
       " 'raifuku': 60,\n",
       " 'veri': 61,\n",
       " 'click': 62,\n",
       " 'jouhou': 63,\n",
       " 'child': 64,\n",
       " 'xiangyao': 65,\n",
       " 'nekko-kun': 66,\n",
       " 'mushibugyo': 67,\n",
       " 'dahua': 68,\n",
       " 'kawaijuku': 69,\n",
       " 'moco': 70,\n",
       " 'd.c.iii': 71,\n",
       " 'structur': 72,\n",
       " 'jim': 73,\n",
       " 'shashinkan': 74,\n",
       " 'shitsurakuen': 75,\n",
       " 'reiteki': 76,\n",
       " 'sailor-fuku': 77,\n",
       " 'orenchi': 78,\n",
       " 'famill': 79,\n",
       " 'like': 80,\n",
       " 'yashahim': 81,\n",
       " 'fuyuyasumi': 82,\n",
       " 'amakakeru': 83,\n",
       " 'utaou': 84,\n",
       " 'high☆spe': 85,\n",
       " 'rabbitub': 86,\n",
       " 'zeninshuugou': 87,\n",
       " 'kitakaz': 88,\n",
       " 'yobikou': 89,\n",
       " 'kuroki': 90,\n",
       " 'fatz': 91,\n",
       " 'shinki': 92,\n",
       " 'spirit': 93,\n",
       " 'seraph': 94,\n",
       " 'aki': 95,\n",
       " 'judo': 96,\n",
       " 'hikaruon': 97,\n",
       " 'tanuki-bayashi': 98,\n",
       " 'wars/forc': 99,\n",
       " 'groov': 100,\n",
       " 'kusog': 101,\n",
       " 'tamageta-kun': 102,\n",
       " 'zhaoqin': 103,\n",
       " 'sola': 104,\n",
       " 'mito': 105,\n",
       " 'ell': 106,\n",
       " 'execut': 107,\n",
       " 'dokuro': 108,\n",
       " 'roommat': 109,\n",
       " 'accomplic': 110,\n",
       " 'tsuzukunja': 111,\n",
       " 'kantarou': 112,\n",
       " 'isogashi': 113,\n",
       " 'shita-kiri': 114,\n",
       " 'cicada': 115,\n",
       " 'escalay': 116,\n",
       " 'bijukubo': 117,\n",
       " 'hareluya': 118,\n",
       " 'clumsi': 119,\n",
       " 'remix': 120,\n",
       " 'sabaku': 121,\n",
       " 'draw': 122,\n",
       " 'furikaeru': 123,\n",
       " 'toozakaru': 124,\n",
       " 'juju': 125,\n",
       " 'azusa': 126,\n",
       " 'crono': 127,\n",
       " 'houjou': 128,\n",
       " 'doku': 129,\n",
       " 'yoko': 130,\n",
       " 'gaik': 131,\n",
       " 'picnic': 132,\n",
       " 'chijoku': 133,\n",
       " 'shougekijou': 134,\n",
       " 'kihon': 135,\n",
       " 'tsuma': 136,\n",
       " 'nyaruko': 137,\n",
       " 'aisuru': 138,\n",
       " 'dolphin': 139,\n",
       " 'otamajakushi': 140,\n",
       " 'furifur': 141,\n",
       " 'biburi': 142,\n",
       " 'uzamaid': 143,\n",
       " 'zai': 144,\n",
       " 'sourou': 145,\n",
       " 'zhengyi': 146,\n",
       " 'koeta': 147,\n",
       " 'kudasai': 148,\n",
       " 'equat': 149,\n",
       " 'hoero': 150,\n",
       " 'nasuno': 151,\n",
       " 'choukoukaku': 152,\n",
       " 'tanita': 153,\n",
       " 'suizokukan': 154,\n",
       " 'genzou': 155,\n",
       " 'daisuugeki': 156,\n",
       " 'shadowvers': 157,\n",
       " 'kotengu': 158,\n",
       " '♭': 159,\n",
       " 'contact': 160,\n",
       " 'judi': 161,\n",
       " 'luan': 162,\n",
       " 'calm': 163,\n",
       " 'chakapoko': 164,\n",
       " 'burglar': 165,\n",
       " 'souhaku': 166,\n",
       " 'zenkoku-hen': 167,\n",
       " 'sunabouzu': 168,\n",
       " 'fuyu→kotatsu←haru': 169,\n",
       " 'swimmi': 170,\n",
       " 'slap': 171,\n",
       " 'soudansho': 172,\n",
       " 'countdown': 173,\n",
       " 'kougunka': 174,\n",
       " 'akashi': 175,\n",
       " 'kyokujitsu': 176,\n",
       " 'bakeri': 177,\n",
       " 'kuufuku': 178,\n",
       " 'joutoku': 179,\n",
       " 'tensura': 180,\n",
       " 'tangguo': 181,\n",
       " 'pokk': 182,\n",
       " 'lull': 183,\n",
       " 'genma': 184,\n",
       " 'teikoku': 185,\n",
       " 'd.gray-man': 186,\n",
       " 'straw-saurus': 187,\n",
       " 'genkina': 188,\n",
       " 'bus': 189,\n",
       " 'ladybird': 190,\n",
       " 'rocco': 191,\n",
       " 'sis': 192,\n",
       " 'tobaso': 193,\n",
       " 'soresaemo': 194,\n",
       " 'jisho': 195,\n",
       " 'yogosu': 196,\n",
       " 'nurs': 197,\n",
       " 'harenchi': 198,\n",
       " 'sorairo': 199,\n",
       " 'okashi': 200,\n",
       " 'dattebayo': 201,\n",
       " 'sume': 202,\n",
       " 'kioka': 203,\n",
       " 'run=dim': 204,\n",
       " 'kingdom': 205,\n",
       " '~after': 206,\n",
       " 'notat': 207,\n",
       " \"'m\": 208,\n",
       " 'zeno': 209,\n",
       " 'shiosai': 210,\n",
       " 'shirokuma': 211,\n",
       " 'druaga': 212,\n",
       " 'batter': 213,\n",
       " 'yadomeshi': 214,\n",
       " 'riversid': 215,\n",
       " 'rowdi': 216,\n",
       " 'joban': 217,\n",
       " 'dainanaka': 218,\n",
       " 'dokyou': 219,\n",
       " 'spyce': 220,\n",
       " 'ulibyeol': 221,\n",
       " 'overbloom': 222,\n",
       " 'hachimitsu': 223,\n",
       " 'yuuhi': 224,\n",
       " 'kenta': 225,\n",
       " 'yonimo': 226,\n",
       " 'teatrino': 227,\n",
       " 'pino': 228,\n",
       " 'ookamida': 229,\n",
       " 'liner': 230,\n",
       " 'lucario': 231,\n",
       " 'maniac': 232,\n",
       " 'gurentai': 233,\n",
       " 'bakabon': 234,\n",
       " 'kubozono': 235,\n",
       " 'gentei': 236,\n",
       " 'mell': 237,\n",
       " 'vanguard': 238,\n",
       " 'takkyuubu': 239,\n",
       " 'diqiu': 240,\n",
       " 'suisougaku-bu': 241,\n",
       " 'ongakukai': 242,\n",
       " 'mage': 243,\n",
       " 'file-x': 244,\n",
       " 'tourism': 245,\n",
       " 'huli': 246,\n",
       " 'tottemo': 247,\n",
       " 'uru': 248,\n",
       " 'luvilia': 249,\n",
       " 'yome-san': 250,\n",
       " 'gunsou': 251,\n",
       " 'kuzu': 252,\n",
       " 'tachiagar': 253,\n",
       " 'amamikoshima': 254,\n",
       " 'dongtian': 255,\n",
       " 'eromam': 256,\n",
       " 'yahari': 257,\n",
       " 'nanahiki': 258,\n",
       " 'shuuchakueki': 259,\n",
       " 'grace': 260,\n",
       " 'misemasu': 261,\n",
       " 'excellion': 262,\n",
       " 'yagi': 263,\n",
       " 'kinkreet': 264,\n",
       " 'zuo': 265,\n",
       " 'ufo': 266,\n",
       " 'nanaana': 267,\n",
       " 'royalti': 268,\n",
       " 'inuyama': 269,\n",
       " 'imperi': 270,\n",
       " 'diao': 271,\n",
       " 'daihanran': 272,\n",
       " 'wengzhong': 273,\n",
       " 'millennia': 274,\n",
       " 'warui': 275,\n",
       " 'kreuz': 276,\n",
       " 'nisha': 277,\n",
       " 'ninjutsu': 278,\n",
       " 'ichizoku': 279,\n",
       " 'short': 280,\n",
       " 'huan': 281,\n",
       " 'monogram': 282,\n",
       " 'sushi': 283,\n",
       " 'tactic': 284,\n",
       " 'madoromi': 285,\n",
       " 'reboot': 286,\n",
       " 'realist': 287,\n",
       " 'baikinman': 288,\n",
       " 'kiit': 289,\n",
       " 'senlin': 290,\n",
       " 'kurenai': 291,\n",
       " 'mechadock': 292,\n",
       " 'monologu': 293,\n",
       " 'yuuna-san': 294,\n",
       " 'sebangou': 295,\n",
       " 'bunna': 296,\n",
       " 'minwa': 297,\n",
       " 'stud': 298,\n",
       " 'entaku': 299,\n",
       " 'sofyrubi': 300,\n",
       " 'kuro-nikuru': 301,\n",
       " 'oraretara': 302,\n",
       " 'fencer': 303,\n",
       " 'kita': 304,\n",
       " 'arcadia-gou': 305,\n",
       " 'drugstor': 306,\n",
       " 'choubatsu': 307,\n",
       " 'er': 308,\n",
       " 'kaseifu': 309,\n",
       " 'monsterz': 310,\n",
       " 'kaikitan': 311,\n",
       " 'funbari': 312,\n",
       " 'portriss': 313,\n",
       " 'town': 314,\n",
       " 'taisenki': 315,\n",
       " 'kanan': 316,\n",
       " 'meifon': 317,\n",
       " 'qicai': 318,\n",
       " 'dalbitkungkwol': 319,\n",
       " 'haedori': 320,\n",
       " 'kegasareru': 321,\n",
       " 'geum': 322,\n",
       " 'max': 323,\n",
       " 'brief': 324,\n",
       " 'senkitan': 325,\n",
       " 'eiken': 326,\n",
       " 'yunh': 327,\n",
       " 'uranof': 328,\n",
       " 'popolocroi': 329,\n",
       " 'benkai': 330,\n",
       " 'choral': 331,\n",
       " 'nakanaori': 332,\n",
       " 'shiteiseki': 333,\n",
       " 'urasekai': 334,\n",
       " 'masaiban': 335,\n",
       " 'tamag': 336,\n",
       " 'hachidori': 337,\n",
       " 'kizuato': 338,\n",
       " 'dept': 339,\n",
       " 'shuzi': 340,\n",
       " 'someon': 341,\n",
       " 'omishi': 342,\n",
       " 'umareta': 343,\n",
       " 'madoka': 344,\n",
       " 'theme': 345,\n",
       " 'shiton': 346,\n",
       " 'asaki': 347,\n",
       " 'pandra': 348,\n",
       " 'claus': 349,\n",
       " 'cruso': 350,\n",
       " 'abe': 351,\n",
       " 'dagram': 352,\n",
       " 'norabbit': 353,\n",
       " 'chicchai': 354,\n",
       " 'itsuka': 355,\n",
       " 'hell': 356,\n",
       " 'tunshi': 357,\n",
       " 'ryuuken': 358,\n",
       " 'yuunagi': 359,\n",
       " 'pelican': 360,\n",
       " 'haritsuk': 361,\n",
       " 'bananajima': 362,\n",
       " 'tsudou': 363,\n",
       " 'momokyun': 364,\n",
       " 'rosario': 365,\n",
       " \"h'or\": 366,\n",
       " 'ryuukoku': 367,\n",
       " 'kokomom': 368,\n",
       " 'smart': 369,\n",
       " 'laid': 370,\n",
       " 'sousamou': 371,\n",
       " 'kokoro-chan': 372,\n",
       " 'yuyushiki': 373,\n",
       " '.koni-chan': 374,\n",
       " 'kagakusha-tachi': 375,\n",
       " 'mihoshi': 376,\n",
       " 'omoshiroi': 377,\n",
       " 'million': 378,\n",
       " 'kageokuri': 379,\n",
       " 'rahmen': 380,\n",
       " 'epic': 381,\n",
       " 'spiral': 382,\n",
       " 'entei': 383,\n",
       " 'fudanshi': 384,\n",
       " 'aah': 385,\n",
       " 'ittokiya': 386,\n",
       " 'erolut': 387,\n",
       " 'juli': 388,\n",
       " 'spriggan': 389,\n",
       " 'assiz': 390,\n",
       " 'arara': 391,\n",
       " 'ppalgan': 392,\n",
       " 'gakusav': 393,\n",
       " 'casshern': 394,\n",
       " 'nihonichi': 395,\n",
       " 'oshushidayo': 396,\n",
       " 'k×drop': 397,\n",
       " 'ometeotl': 398,\n",
       " 'beani': 399,\n",
       " 'doubutsu': 400,\n",
       " 'anoth': 401,\n",
       " 'sama': 402,\n",
       " 'kangoku': 403,\n",
       " 'aldnoah.zero': 404,\n",
       " 'ouma': 405,\n",
       " 'rhea': 406,\n",
       " 'elvi': 407,\n",
       " 'axi': 408,\n",
       " 'shougakusei': 409,\n",
       " 'gaist': 410,\n",
       " 'cubix': 411,\n",
       " 'tcg': 412,\n",
       " 'fuiteru': 413,\n",
       " 'the☆doraemon': 414,\n",
       " 'cazador': 415,\n",
       " 'dougo': 416,\n",
       " 'kashiwa': 417,\n",
       " 'petro': 418,\n",
       " 'fushigi': 419,\n",
       " 'moeru': 420,\n",
       " 'yan': 421,\n",
       " 'paripari': 422,\n",
       " 'kiniitta': 423,\n",
       " 'u.c': 424,\n",
       " 'korekarasaki': 425,\n",
       " 'koet': 426,\n",
       " 'eikoden': 427,\n",
       " 'bookmark': 428,\n",
       " 'cesar': 429,\n",
       " 'eightraid': 430,\n",
       " 'waratt': 431,\n",
       " 'rainbowman': 432,\n",
       " 'patron': 433,\n",
       " 'eiga-sai': 434,\n",
       " 'zuma': 435,\n",
       " 'tatta': 436,\n",
       " 'fujiyama': 437,\n",
       " 'nobushi': 438,\n",
       " 'tamago': 439,\n",
       " 'satisfact': 440,\n",
       " 'jiisan': 441,\n",
       " 'inspector': 442,\n",
       " 'nyuu': 443,\n",
       " 'jimiko': 444,\n",
       " 'keito': 445,\n",
       " 'bewar': 446,\n",
       " 'tanteidokoro': 447,\n",
       " 'awak': 448,\n",
       " 'asatarou': 449,\n",
       " 'mura': 450,\n",
       " 'nukiuchi': 451,\n",
       " 'jeog-en': 452,\n",
       " 'lagoon': 453,\n",
       " 'chikku': 454,\n",
       " 'biyaku': 455,\n",
       " 'afghanistan': 456,\n",
       " 'revevolut': 457,\n",
       " 'tie': 458,\n",
       " 'majutsu': 459,\n",
       " 'oyayubi-him': 460,\n",
       " 'mignon': 461,\n",
       " 'mamma★mia': 462,\n",
       " 'carlvinson': 463,\n",
       " 'talent': 464,\n",
       " 'nanoru': 465,\n",
       " 'guilai': 466,\n",
       " 'chawan': 467,\n",
       " 'dairinin': 468,\n",
       " 'chiyoko': 469,\n",
       " 'shenzhou': 470,\n",
       " 'korakuen': 471,\n",
       " 'coaster': 472,\n",
       " 'tsum': 473,\n",
       " 'elmer': 474,\n",
       " 'matsuei': 475,\n",
       " 'kotaro': 476,\n",
       " 'kabushiki': 477,\n",
       " 'yeohaengja': 478,\n",
       " 'taoqi': 479,\n",
       " 'gochou': 480,\n",
       " 'owakare-kai': 481,\n",
       " 'wanghu': 482,\n",
       " 'tomoyo-hen': 483,\n",
       " 'konohamaru': 484,\n",
       " 'tsuki-sama': 485,\n",
       " 'oheya': 486,\n",
       " 'break-ag': 487,\n",
       " 'chiguhagu': 488,\n",
       " 'tenjhoteng': 489,\n",
       " 'shikoyaka': 490,\n",
       " 'melti': 491,\n",
       " 'baraou': 492,\n",
       " 'plawr': 493,\n",
       " 'heidi': 494,\n",
       " 'otoboku': 495,\n",
       " 'jutai': 496,\n",
       " 'ooi': 497,\n",
       " 'natsukashii': 498,\n",
       " 'ryouriten': 499,\n",
       " 'tobasu': 500,\n",
       " 'todoufuken': 501,\n",
       " 'kedo': 502,\n",
       " 'echigo': 503,\n",
       " 'ball': 504,\n",
       " 'kusuo': 505,\n",
       " 'yuki-chan': 506,\n",
       " 'shabak': 507,\n",
       " 'ojaru': 508,\n",
       " 'debt': 509,\n",
       " 'yin': 510,\n",
       " 'mezashita': 511,\n",
       " 'bo': 512,\n",
       " 'otoridai': 513,\n",
       " 'baenang-yeohaeng': 514,\n",
       " 'strawberri': 515,\n",
       " 'street': 516,\n",
       " 'zhu': 517,\n",
       " 'mourn': 518,\n",
       " 'juushi': 519,\n",
       " 'lulu': 520,\n",
       " 'urmă': 521,\n",
       " 'kayou': 522,\n",
       " 'hanazono': 523,\n",
       " 'detroit': 524,\n",
       " 'tuzki': 525,\n",
       " 'kyutai': 526,\n",
       " 'yewai': 527,\n",
       " 'kikuko': 528,\n",
       " 'kitsutsuki': 529,\n",
       " 'byoutou': 530,\n",
       " 'oosouji': 531,\n",
       " 'medley': 532,\n",
       " 'edg': 533,\n",
       " 'pierrot': 534,\n",
       " 'poroporo': 535,\n",
       " 'mushakero': 536,\n",
       " 'ontama': 537,\n",
       " 'chizuru-chan': 538,\n",
       " 'cord': 539,\n",
       " 'jameseo': 540,\n",
       " 'jobuubu': 541,\n",
       " 'shidoushit': 542,\n",
       " 'jaku-chara': 543,\n",
       " 'daojian': 544,\n",
       " 'kowaku': 545,\n",
       " 'crew': 546,\n",
       " 'stella': 547,\n",
       " 'tenta': 548,\n",
       " 'let': 549,\n",
       " 'ganbaruzu': 550,\n",
       " 'kindergarten': 551,\n",
       " 'kosi': 552,\n",
       " 'seieki♥': 553,\n",
       " 'taerang': 554,\n",
       " 'maetel': 555,\n",
       " 'bakegyamon': 556,\n",
       " 'shoukoujo': 557,\n",
       " 'ayane-chan': 558,\n",
       " 'mortal': 559,\n",
       " 'kun': 560,\n",
       " 'ijim': 561,\n",
       " 'nanbo': 562,\n",
       " 'hanbun': 563,\n",
       " 'kuukijuu': 564,\n",
       " 'junk': 565,\n",
       " 'ggo': 566,\n",
       " 'mina': 567,\n",
       " 'daniel': 568,\n",
       " 'sam': 569,\n",
       " 'nendjuugyouji': 570,\n",
       " 'extra-terrestri': 571,\n",
       " 'douryoku': 572,\n",
       " 'atta': 573,\n",
       " 'newmanoid': 574,\n",
       " 'nostradamus': 575,\n",
       " 'suzakinishi': 576,\n",
       " 'mohuan': 577,\n",
       " 'tadaiku': 578,\n",
       " 'maman': 579,\n",
       " 'jiru': 580,\n",
       " 'arthur': 581,\n",
       " 'inkou': 582,\n",
       " 'reconcili': 583,\n",
       " 'tsunami': 584,\n",
       " 'hoshiai': 585,\n",
       " 'greed': 586,\n",
       " 'nareru': 587,\n",
       " 'cuticl': 588,\n",
       " 'warenai': 589,\n",
       " 'ashiaraiyashiki': 590,\n",
       " 'yumearu': 591,\n",
       " 'calpi': 592,\n",
       " 'lite': 593,\n",
       " 'hiraku': 594,\n",
       " 'housnail': 595,\n",
       " 'keshiko': 596,\n",
       " 'itadaki': 597,\n",
       " 'koeru': 598,\n",
       " 'dokkaebi': 599,\n",
       " 'gloria': 600,\n",
       " 'sasami': 601,\n",
       " 'gakushou': 602,\n",
       " 'annai': 603,\n",
       " 'nibelung': 604,\n",
       " 'mianhada': 605,\n",
       " 'rifujin-jin': 606,\n",
       " 'superband': 607,\n",
       " 'keikaku': 608,\n",
       " 'colocolo': 609,\n",
       " 'naruto': 610,\n",
       " 'kaleido': 611,\n",
       " 'hakken': 612,\n",
       " 'luna': 613,\n",
       " 'binchou-tan': 614,\n",
       " 'nusumareta': 615,\n",
       " 'tokei': 616,\n",
       " 'dededen': 617,\n",
       " 'rurouni': 618,\n",
       " 'plane': 619,\n",
       " 'brother': 620,\n",
       " 'touhoku-hen': 621,\n",
       " 'ryohei': 622,\n",
       " 'blossom~': 623,\n",
       " 'sad': 624,\n",
       " 'minai': 625,\n",
       " 'gilgamesh': 626,\n",
       " 'tomoko': 627,\n",
       " 'pappara': 628,\n",
       " 'ikikata': 629,\n",
       " 'konsui': 630,\n",
       " 'sakura': 631,\n",
       " 's.t.a.r.t': 632,\n",
       " 'kyakkya': 633,\n",
       " 'senko-san': 634,\n",
       " 'nymphomaniac': 635,\n",
       " 'ralli': 636,\n",
       " 'kakushi': 637,\n",
       " 'itou': 638,\n",
       " 'chitose-hen': 639,\n",
       " 'unison': 640,\n",
       " 'suru': 641,\n",
       " 'shigutsman': 642,\n",
       " 'hanaukyou': 643,\n",
       " 'fumai-kou': 644,\n",
       " 'chuutou-bu': 645,\n",
       " 'yotoden': 646,\n",
       " 'okashina': 647,\n",
       " 'imari': 648,\n",
       " 'dommel': 649,\n",
       " 'hako': 650,\n",
       " 'robo-rumbl': 651,\n",
       " 'genocyb': 652,\n",
       " 'xuzhang': 653,\n",
       " 'iwatobi': 654,\n",
       " 'shake': 655,\n",
       " 'suttoko': 656,\n",
       " 'unsu': 657,\n",
       " 'danganronpa': 658,\n",
       " 'chai': 659,\n",
       " 'futakomori': 660,\n",
       " 'salamand': 661,\n",
       " 'shuto': 662,\n",
       " 'hiros': 663,\n",
       " 'moero': 664,\n",
       " 'dogiragon': 665,\n",
       " 'beullaeg-eosseolteuui': 666,\n",
       " 'teardrop': 667,\n",
       " 'chengxia': 668,\n",
       " 'darken': 669,\n",
       " 'nuigurumi': 670,\n",
       " 'york': 671,\n",
       " 'shining☆rom': 672,\n",
       " 'gali': 673,\n",
       " 'privat': 674,\n",
       " 'y': 675,\n",
       " 'lue': 676,\n",
       " 'bakukyuu': 677,\n",
       " 'hikiko': 678,\n",
       " 'elus': 679,\n",
       " 'shinemon-hen': 680,\n",
       " 'zhipai': 681,\n",
       " 'g-best': 682,\n",
       " 'mametarou': 683,\n",
       " 'kobuta': 684,\n",
       " 'konogoro': 685,\n",
       " 'fog': 686,\n",
       " 'ippun': 687,\n",
       " 'corn': 688,\n",
       " 'gyo': 689,\n",
       " 'robokko': 690,\n",
       " 'chokkyuu': 691,\n",
       " 'yuujinchou': 692,\n",
       " 'googuri': 693,\n",
       " 'saiyajin': 694,\n",
       " 'viewti': 695,\n",
       " 'b.e.e': 696,\n",
       " 'mizugi': 697,\n",
       " 'kininatt': 698,\n",
       " 'ma-eum-uisoli': 699,\n",
       " 'tsuukin': 700,\n",
       " 'kkaeeoboni': 701,\n",
       " 'katasumi': 702,\n",
       " 'onnanoko': 703,\n",
       " 'ijirar': 704,\n",
       " 'chiharu': 705,\n",
       " 'deji': 706,\n",
       " 'arpo': 707,\n",
       " 'sunghyungsoo': 708,\n",
       " 'poka': 709,\n",
       " 'baldr': 710,\n",
       " 'oishi-sa': 711,\n",
       " 'takarabako': 712,\n",
       " 'chunfeng': 713,\n",
       " 'reimeiki': 714,\n",
       " 'meisei': 715,\n",
       " 'away': 716,\n",
       " 'sloven': 717,\n",
       " 'disney': 718,\n",
       " 'miyori': 719,\n",
       " \"girl's-fram\": 720,\n",
       " 'joybo': 721,\n",
       " 'mijikamon': 722,\n",
       " 'milk-chan': 723,\n",
       " 'koushi': 724,\n",
       " 'tennin': 725,\n",
       " 'teugsu': 726,\n",
       " 'kowashichatta': 727,\n",
       " 'tsuigeki': 728,\n",
       " 'puchim': 729,\n",
       " 'sansan': 730,\n",
       " 'sc': 731,\n",
       " 'champloo': 732,\n",
       " 'yinhun': 733,\n",
       " 'fumutto': 734,\n",
       " 'gordian': 735,\n",
       " 'geba': 736,\n",
       " 'paprika': 737,\n",
       " 'tankoufu': 738,\n",
       " '~su~mi~re~': 739,\n",
       " 'qingk': 740,\n",
       " 'linebarrel': 741,\n",
       " 'kattobi': 742,\n",
       " 'mazica': 743,\n",
       " 'jianianhua': 744,\n",
       " 'ingan': 745,\n",
       " 'shimauma': 746,\n",
       " 'aosa': 747,\n",
       " 'tokio': 748,\n",
       " 'macademi': 749,\n",
       " 'sonata': 750,\n",
       " 'dangorou': 751,\n",
       " 'wedding★combat': 752,\n",
       " 'shoujo☆kageki': 753,\n",
       " 'irokoi': 754,\n",
       " 'jigueseo': 755,\n",
       " 'angeltia': 756,\n",
       " 'discipl': 757,\n",
       " 'owari': 758,\n",
       " 'art': 759,\n",
       " 'kagerouka-kun': 760,\n",
       " 'hi.me.go.to': 761,\n",
       " 'doe': 762,\n",
       " 'gatenkei': 763,\n",
       " 'ke': 764,\n",
       " 'togainu': 765,\n",
       " 'ashi': 766,\n",
       " 'u-jin': 767,\n",
       " 'tong': 768,\n",
       " 'nashi': 769,\n",
       " 'naxi': 770,\n",
       " 'rilu': 771,\n",
       " 'gad': 772,\n",
       " 'chougekisen': 773,\n",
       " 'gendaijuu': 774,\n",
       " 'robocchi': 775,\n",
       " 'oraa': 776,\n",
       " 'bougyakusareta': 777,\n",
       " 'loui': 778,\n",
       " 'maxvalu': 779,\n",
       " 'o-o-o': 780,\n",
       " 'laura': 781,\n",
       " 'tumbl': 782,\n",
       " 'immort': 783,\n",
       " 'keiraku': 784,\n",
       " 'dwarf': 785,\n",
       " 'imakara': 786,\n",
       " 'sequenc': 787,\n",
       " 'muket': 788,\n",
       " 'mechapoli': 789,\n",
       " 'campanella': 790,\n",
       " 'makemag': 791,\n",
       " 'handsom': 792,\n",
       " 'makibao': 793,\n",
       " 'hanasakeru': 794,\n",
       " 'bone': 795,\n",
       " 'hijouji': 796,\n",
       " 'lia': 797,\n",
       " 'push': 798,\n",
       " 'phantomi': 799,\n",
       " 'virgin': 800,\n",
       " 'classmat': 801,\n",
       " 'kawarazaki-k': 802,\n",
       " 'nichijou-kei': 803,\n",
       " 'paizuri': 804,\n",
       " 'watashitolipton': 805,\n",
       " 'shinran': 806,\n",
       " 'yourself': 807,\n",
       " 'yomeiri': 808,\n",
       " 'yukiwatari': 809,\n",
       " 'harukaz': 810,\n",
       " 'dorami-chan': 811,\n",
       " 'mimi': 812,\n",
       " 'lover': 813,\n",
       " 'ekubo': 814,\n",
       " 'frontlin': 815,\n",
       " 'futarikiri': 816,\n",
       " 'laugh': 817,\n",
       " 'gargoyl': 818,\n",
       " 'higenashi': 819,\n",
       " 'bijutsubu': 820,\n",
       " 'mmo': 821,\n",
       " 'kota': 822,\n",
       " 'ergo': 823,\n",
       " 'ippai/boku': 824,\n",
       " 'badland': 825,\n",
       " 'sansha': 826,\n",
       " 'wolf': 827,\n",
       " 'key': 828,\n",
       " 'shiguang': 829,\n",
       " 'fragtim': 830,\n",
       " 'size': 831,\n",
       " 'bottom': 832,\n",
       " 'mcdull': 833,\n",
       " 'teleclub': 834,\n",
       " 'fuichin-san': 835,\n",
       " 'proclam': 836,\n",
       " 'nyanbo': 837,\n",
       " 'zuozhan': 838,\n",
       " 'kidzuki-hen': 839,\n",
       " 'dick': 840,\n",
       " 'uroko': 841,\n",
       " 'golfer': 842,\n",
       " 'tasuuketsu': 843,\n",
       " 'midori-ppoi': 844,\n",
       " 'betray': 845,\n",
       " 'kemono-tachi': 846,\n",
       " 'kakuchou': 847,\n",
       " 'kyouiku': 848,\n",
       " 'gai-hen': 849,\n",
       " 'juan': 850,\n",
       " 'kamigami': 851,\n",
       " 'lio': 852,\n",
       " 'baraki': 853,\n",
       " 'begin': 854,\n",
       " 'natsu-iro': 855,\n",
       " 'disguis': 856,\n",
       " 'kuruoshii': 857,\n",
       " 'proxi': 858,\n",
       " 'ushigaeru': 859,\n",
       " 're-main': 860,\n",
       " 'sugi': 861,\n",
       " 'darkstalk': 862,\n",
       " 'umezu': 863,\n",
       " 'iuna': 864,\n",
       " 'mutou': 865,\n",
       " 'kyoui': 866,\n",
       " 'shoshi': 867,\n",
       " 'mamonogatari': 868,\n",
       " 'mayb': 869,\n",
       " 'himitsu': 870,\n",
       " 'katsut': 871,\n",
       " 'shitennou': 872,\n",
       " 'kippu': 873,\n",
       " 'rika': 874,\n",
       " 'gts': 875,\n",
       " 'gensou-teki': 876,\n",
       " 'here': 877,\n",
       " 'tesapuru': 878,\n",
       " 'gokkun': 879,\n",
       " 'yanyi': 880,\n",
       " 'harri': 881,\n",
       " 'miteen': 882,\n",
       " 'dwaeji-ui': 883,\n",
       " 'bell': 884,\n",
       " 'iruka': 885,\n",
       " 'onshuu': 886,\n",
       " 'brynhildr': 887,\n",
       " 'ascens': 888,\n",
       " 'yanbou': 889,\n",
       " 'sweet': 890,\n",
       " 'sasuk': 891,\n",
       " 'crayon': 892,\n",
       " 'templex': 893,\n",
       " 'kimigayo': 894,\n",
       " 'moribito': 895,\n",
       " 'reideen': 896,\n",
       " 'kairiku': 897,\n",
       " 'narimasu': 898,\n",
       " 'cup': 899,\n",
       " 'doeeo': 900,\n",
       " 'gerent': 901,\n",
       " 'ichigo': 902,\n",
       " 'xiaojiang': 903,\n",
       " 'wata': 904,\n",
       " 'shiru': 905,\n",
       " 'an': 906,\n",
       " 'saranghanda': 907,\n",
       " 'bouquet': 908,\n",
       " 'toyo': 909,\n",
       " 'shinnyuusei': 910,\n",
       " 'manabi': 911,\n",
       " 'pud': 912,\n",
       " 'bakuen': 913,\n",
       " 'quartet': 914,\n",
       " 'loll': 915,\n",
       " 'xianluo': 916,\n",
       " 'be-boy': 917,\n",
       " 'scar': 918,\n",
       " 'kouryaku': 919,\n",
       " 'cultiv': 920,\n",
       " 'zange-roku': 921,\n",
       " 'nintama': 922,\n",
       " 'momiji': 923,\n",
       " 'begonia': 924,\n",
       " 'harimogu': 925,\n",
       " 'niplheim': 926,\n",
       " 'musubou': 927,\n",
       " 'youko': 928,\n",
       " 'nemuri': 929,\n",
       " 'asia': 930,\n",
       " 'kahey': 931,\n",
       " 'ichigeki': 932,\n",
       " 'shoukai': 933,\n",
       " 'yao': 934,\n",
       " 'möbius': 935,\n",
       " 'grid': 936,\n",
       " 'transfer': 937,\n",
       " 'sotsugyou': 938,\n",
       " 'ryuuseiki': 939,\n",
       " 'chen-kuro': 940,\n",
       " 'order': 941,\n",
       " 'handkerchief': 942,\n",
       " 'aeioudan': 943,\n",
       " 'kobo-chan': 944,\n",
       " 'isshitsu': 945,\n",
       " 'render': 946,\n",
       " 'magical☆shop': 947,\n",
       " 'agasa': 948,\n",
       " 'kitenakut': 949,\n",
       " 'reset': 950,\n",
       " 'utopa': 951,\n",
       " 'mother': 952,\n",
       " 'eren': 953,\n",
       " 'jimao': 954,\n",
       " 'schwarz': 955,\n",
       " 'ghast': 956,\n",
       " 'cogimyun': 957,\n",
       " 'alo': 958,\n",
       " 'voodooland': 959,\n",
       " 'iburi': 960,\n",
       " 'bunkiten': 961,\n",
       " 'rurou': 962,\n",
       " 'tonagura': 963,\n",
       " 'sewayaki': 964,\n",
       " 'domin': 965,\n",
       " 'virtuacal': 966,\n",
       " 'kyokashou': 967,\n",
       " 'miyano': 968,\n",
       " 'miracl': 969,\n",
       " 'samia': 970,\n",
       " 'leyuan': 971,\n",
       " 'rhythm/trump': 972,\n",
       " 'tsukeyou': 973,\n",
       " 'gundam': 974,\n",
       " 'viral': 975,\n",
       " 'tokorozawa': 976,\n",
       " 'willi': 977,\n",
       " 'time-patrol': 978,\n",
       " 'confront': 979,\n",
       " 'ant': 980,\n",
       " 'shirouto': 981,\n",
       " 'makaryuudo': 982,\n",
       " 'escaflown': 983,\n",
       " 'flow': 984,\n",
       " 'douga': 985,\n",
       " 'femal': 986,\n",
       " 'souiu': 987,\n",
       " 'juusenki': 988,\n",
       " 'kanchou': 989,\n",
       " 'lump': 990,\n",
       " 'masani': 991,\n",
       " 'kaitai': 992,\n",
       " 'shitou-hen': 993,\n",
       " 'purin': 994,\n",
       " 'aliv': 995,\n",
       " 'hiza': 996,\n",
       " 'fushisha': 997,\n",
       " 'choujigen': 998,\n",
       " 'spotlight': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "voc_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic question\n",
    "You consult for a personal trainer who has a back-to-back sequence of requests for appointments. A sequence of requests is of the form > 30, 40, 25, 50, 30, 20 where each number is the time that the person who makes the appointment wants to spend. You need to accept some requests, however you need a break between them, so you cannot accept two consecutive requests. For example, [30, 50, 20] is an acceptable solution (of duration 100), but [30, 40, 50, 20] is not, because 30 and 40 are two consecutive appointments. Your goal is to provide to the personal trainer a schedule that maximizes the total length of the accepted appointments. For example, in the previous instance, the optimal solution is [40, 50, 20], of total duration 110.\n",
    "\n",
    "* Write an algorithm that computes the acceptable solution with the longest possible duration.\n",
    "* Implement a program that given in input an instance in the form given above, gives the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoritmo (array):\n",
    "    a=0\n",
    "    b=array[0]\n",
    "    for elem in array[1:]:\n",
    "        n=max(a+elem,b)\n",
    "        a=b\n",
    "        b=n\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array=[3,4,5,60,4]\n",
    "algoritmo(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0e40e7c7a66e87c69eaa7498d7778a1d8fa6b3e422091d0b3e8dafd8f730247"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
